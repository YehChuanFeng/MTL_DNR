{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 1\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "max_epoch = 50\n",
    "experiment_time = 1\n",
    "limit_early_stop_count = 5\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "use_upsample = True\n",
    "use_mini_sample = True\n",
    "\n",
    "#task_name_list = ['dialysis','DNR']\n",
    "#task_name_list = ['dod_7day','DNR']\n",
    "#task_name_list = ['dod_30day','DNR']\n",
    "#task_name_list = ['dod_90day','DNR']\n",
    "#task_name_list = ['Weaning_successful','DNR']\n",
    "#task_name_list = ['SBT','DNR']\n",
    "#task_name_list = ['dod_30day']\n",
    "#task_name_list = ['dod','dod_90day']\n",
    "#task_name_list = ['dod_30day','dod_90day']\n",
    "#task_name_list = ['dod']\n",
    "#task_name_list = ['DNR']\n",
    "\n",
    "task_name_list = ['DNR','dod_30day','Vasopressor']\n",
    "\n",
    "#task_name_list = ['DNR','dod_30day','Vasopressor','InvasiveVent','dialysis']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "##++\n",
    "class RNN_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, task_name_list,window_size = 3, dropout_ratio=0.0):\n",
    "        super(RNN_MTL, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.relu = nn.ReLU()  # Activation function for hidden layers\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.task_name_list = task_name_list\n",
    "        self.num_tasks = len(task_name_list)\n",
    "        \n",
    "        #20240717\n",
    "        self.n_temporal = 0\n",
    "        self.n_static = 0\n",
    "        \n",
    "        \n",
    "        hidden_dim = [256, 64]\n",
    "        output_size = 1\n",
    "\n",
    "        # Bottom\n",
    "        self.bi_lstm = torch.nn.LSTM(input_dim, hidden_dim[0], num_layers=2, batch_first = True, bidirectional = True)\n",
    "        #self.bt_fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        #self.bt_fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        #self.bt_fc3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
    "\n",
    "        # Towers\n",
    "        self.task_fc0 = nn.ModuleList([nn.Linear(hidden_dim[0]*window_size*2, hidden_dim[1]) for _ in range(self.num_tasks)])\n",
    "        self.task_fc1 = nn.ModuleList([nn.Linear(hidden_dim[1], output_size) for _ in range(self.num_tasks)])\n",
    "    \n",
    "        #Towers test\n",
    "        # self.task_fc0 = nn.ModuleList([nn.Linear(hidden_dim[0] * window_size * 2, hidden_dim[1]) for _ in range(self.num_tasks)])\n",
    "        # self.task_fc1 = nn.ModuleList([nn.Linear(hidden_dim[1], output_size) for _ in range(self.num_tasks)])\n",
    "\n",
    "    def data_check(self,x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        #if x.ndim == 3:\n",
    "        #    x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])  # Flatten \n",
    "        \n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.train() \n",
    "        \n",
    "        #20240717\n",
    "        if x.ndim == 2:\n",
    "            x = x.reshape(x.shape[0], 3, self.n_temporal+self.n_static)\n",
    "            \n",
    "        x = self.data_check(x)\n",
    "        \n",
    "        h, _   = self.bi_lstm(x)\n",
    "        h = torch.nn.functional.relu(h)\n",
    "        h = torch.flatten(h, start_dim = 1)\n",
    "        # Towers\n",
    "        task_out = {}\n",
    "        for task_index in range(self.num_tasks):\n",
    "            task_name = self.task_name_list[task_index]\n",
    "            hi = self.task_fc0[task_index](h)\n",
    "            hi = self.relu(hi)\n",
    "            hi = self.dropout(hi)\n",
    "            hi = self.task_fc1[task_index](hi)\n",
    "            hi = self.sigmoid(hi)\n",
    "            task_out[task_name] = hi    \n",
    "            \n",
    "        if len(self.task_name_list) == 1:\n",
    "            return task_out[self.task_name_list[0]]\n",
    "        else:\n",
    "            return task_out\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        return prob_dict\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        \n",
    "        return prob_dict\n",
    "    \n",
    "    def predict(self, x, threshold = 0.5):\n",
    "        self.eval()\n",
    "        prob_dict = self.predict_prob(x)\n",
    "        pred_dict = {}\n",
    "        \n",
    "        for key, value in prob_dict.items():\n",
    "            #tensor轉numpy\n",
    "            value = value.cpu().detach().numpy()\n",
    "            pred_class = [1 if x > threshold else 0 for x in value]\n",
    "            pred_dict[key] = np.array(pred_class) \n",
    "        return pred_dict\n",
    "    \n",
    "    def evaluate(self,X,label,task_name,criterion):\n",
    "        with torch.no_grad():\n",
    "            prob = self.predict_prob(X)[task_name].cpu().detach().numpy() #tensor=>numpy\n",
    "            pred = self.predict(X)[task_name] \n",
    "            score = compute_scores(label,pred,prob)\n",
    "            score['task'] = task_name\n",
    "            loss = criterion(torch.from_numpy(prob).to(device),torch.from_numpy(label).to(device)).item()\n",
    "            score['loss'] = loss/len(label)\n",
    "            return score\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8537c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y_true, y_pred,y_prob):\n",
    "    if np.any(np.isnan(y_prob)):\n",
    "        print(y_prob)\n",
    "        input()\n",
    "        \n",
    "    scores = {}\n",
    "    try:\n",
    "        scores['task'] = 'Null'\n",
    "        scores['auroc'] = round(roc_auc_score(y_true, y_prob), 3)\n",
    "        scores['acc'] = round(accuracy_score(y_true, y_pred), 3)\n",
    "        scores['f1'] = round(f1_score(y_true, y_pred), 3)\n",
    "        scores['pre'] = round(precision_score(y_true, y_pred), 3)\n",
    "        scores['recall'] = round(recall_score(y_true, y_pred), 3)\n",
    "        scores['brier_score'] = round(brier_score_loss(y_true, y_prob), 3)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    model\n",
    "    dict: Mydataset\n",
    "    loss_function\n",
    "Output:\n",
    "    score: dict + dict\n",
    "    result: dict => ['total_auc','total_loss']\n",
    "\"\"\"\n",
    "def test(model, dataset_dict, criterion, is_show = True):\n",
    "    model.eval()\n",
    "\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    for task_name in task_name_list:  # 循環每個任務\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "        score[task_name] = model.evaluate(X,Y,task_name,criterion)\n",
    "        \n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "            \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "    \n",
    "    return score,result\n",
    "\n",
    "\"\"\"\n",
    "local_best_model_dict: #dict{'task_name':{'model','performance(target_score)','id'}}\n",
    "model\n",
    "\"\"\"\n",
    "def test2(local_best_model_dict, dataset_dict, criterion, is_show = True):\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    \n",
    "    for task_name in task_name_list:\n",
    "        #modelr.load_state_dict(local_best_model_dict[task_name]['model'])\n",
    "        modelr = local_best_model_dict[task_name]['model']\n",
    "        modelr.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        score[task_name] = modelr.evaluate(X,Y,task_name,criterion)\n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "        \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "    return score,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d074fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, np_X_scalar,np_X_original, np_Y):\n",
    "        self.inputs = torch.from_numpy(np_X_scalar).float()\n",
    "        self.inputs_original = torch.from_numpy(np_X_original).float()\n",
    "        self.labels = torch.from_numpy(np_Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, _input, target):\n",
    "        pt = _input\n",
    "        alpha = self.alpha\n",
    "        loss = - alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1 - alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "def check_label_distribution (data_Y):\n",
    "    count_1 = np.count_nonzero(data_Y == 1)\n",
    "    count_0 = np.count_nonzero(data_Y == 0)\n",
    "    count_others = np.count_nonzero((data_Y != 1) & (data_Y != 0))\n",
    "    ratio_1 = round(count_1/len(data_Y)*100,2)\n",
    "    ratio_0 = round(count_0/len(data_Y)*100,2)\n",
    "    ratio_others = round(count_others/len(data_Y)*100,2)\n",
    "    print(f'Distribution: 1=>{count_1}({ratio_1}%),  0=>{count_0}({ratio_0}%),  others=>{count_others}({ratio_others}%)')\n",
    "\n",
    "    \n",
    "def upsampling_auto(X,X_original,Y,up_ratio):\n",
    "    check_label_distribution(Y)\n",
    "    zero_idx = np.where(Y == 0)[0]\n",
    "    one_idx = np.where(Y == 1)[0]\n",
    "    other_idx = np.where((Y != 1) & (Y != 0))[0]\n",
    "    if len(other_idx > 0):\n",
    "        return X,Y\n",
    "    repeated_data_X = np.tile(X[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_X_original = np.tile(X_original[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_Y = np.tile(Y[one_idx], (up_ratio))\n",
    "\n",
    "    X_upsampled = np.vstack((X[zero_idx], repeated_data_X))\n",
    "    X_original_upsampled = np.vstack((X_original[zero_idx], repeated_data_X_original))\n",
    "\n",
    "    Y_upsampled = np.concatenate((Y[zero_idx], repeated_data_Y)) \n",
    "    return X_upsampled,X_original_upsampled, Y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dde32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: numpy\n",
    "    feature_name_list : List\n",
    "    select_feature_list : List   (必須是feature_name_list的子集)\n",
    "Output\n",
    "    select_feature_list data\n",
    "\"\"\"\n",
    "def select_features(X, feature_name_list, select_feature_list):\n",
    "    invalid_features = set(select_feature_list) - set(feature_name_list)\n",
    "    if invalid_features:\n",
    "        raise ValueError(f\"Invalid features in select_feature_list: {invalid_features}\")\n",
    "    selected_feature_indices = [feature_name_list.index(feature) for feature in select_feature_list]\n",
    "    X_selected = X[:, :, selected_feature_indices]\n",
    "\n",
    "    return X_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958621ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    dataset_dict: Mydataset \n",
    "    loader_dict: Dataloader\n",
    "    feature_name_list: List\n",
    "    select_feature_list: List \n",
    "    batch_size: 256\n",
    "\n",
    "Output:\n",
    "    dataset_dict\n",
    "    loader_dict\n",
    "    feature_name_list ==>\n",
    "    +++\n",
    "\"\"\"\n",
    "\n",
    "def read_data(task_name_list, data_date ,data_type, select_feature_list = [], batch_size = 256,use_upsample = False):\n",
    "    #data_path = \"./data/sample/standard_data\"\n",
    "    data_path = \"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/standard_data\"\n",
    "    \n",
    "    #20241002\n",
    "    #記得要改!!\n",
    "    # n_temporal = 128\n",
    "    # n_static = 143\n",
    "\n",
    "    #20250507\n",
    "    n_temporal = 126\n",
    "    n_static = 145\n",
    "    \n",
    "    #Feature name\n",
    "    #df_feature = pd.read_csv(\"./data/sample/full_feature_name.csv\")\n",
    "    df_feature = pd.read_csv(\"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/full_feature_name.csv\")\n",
    "    feature_name_list = df_feature.columns.to_list()\n",
    "    \n",
    "    #Dataset\n",
    "    dataset_dict = {}\n",
    "    original_data_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        X_scalar = np.load(f\"{data_path}/{data_type}_scalar_X_{task_name}.npy\", allow_pickle=True)\n",
    "        X_original = np.load(f\"{data_path}/{data_type}_X_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        #20240717 攤平\n",
    "        X_scalar = X_scalar.reshape(X_scalar.shape[0], (n_temporal+n_static)*3)\n",
    "        X_original = X_original.reshape(X_original.shape[0], (n_temporal+n_static)*3)\n",
    "        \"\"\"\n",
    "        if len(select_feature_list)>0:\n",
    "            X_scalar = select_features(X_scalar,feature_name_list,select_feature_list)\n",
    "            X_original = select_features(X_original,feature_name_list,select_feature_list)\n",
    "            assert X_scalar.shape[2] == len(select_feature_list)\n",
    "            assert X_original.shape[2] == len(select_feature_list)\n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "\n",
    "        Y = np.load(f\"{data_path}/{data_type}_Y_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        \"\"\"\n",
    "        if use_upsample:\n",
    "            if task_name == 'DNR' and data_type != 'test':\n",
    "                X_scalar,X_original,Y = upsampling_auto(X_scalar,X_original,Y,10)\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset_dict[task_name] = MyDataset(X_scalar,X_original,Y)\n",
    "        original_data_dict['X_scalar'] = X_scalar\n",
    "        original_data_dict['X'] = X_original\n",
    "        \n",
    "        original_data_dict['Y'] = Y\n",
    "        \n",
    "    #Dataloader\n",
    "    loader_dict = {}\n",
    "    for key, dataset in dataset_dict.items():        \n",
    "        loader_dict[key] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataset_dict,loader_dict,feature_name_list,original_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70496de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTL_to_STL(multi_task_model):\n",
    "    single_task_models = {}\n",
    "\n",
    "    for task_index, task_name in enumerate(multi_task_model.task_name_list):\n",
    "        \n",
    "        single_task_model = RNN_MTL(input_dim, [task_name])  \n",
    "        single_task_model.to(device) \n",
    "\n",
    "        #Bottom\n",
    "        # 複製bi_lstm的權重和偏差到model2的對應層中\n",
    "        single_task_model.bi_lstm.weight_ih_l0.data = multi_task_model.bi_lstm.weight_ih_l0.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l0.data = multi_task_model.bi_lstm.weight_hh_l0.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l0.data = multi_task_model.bi_lstm.bias_ih_l0.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l0.data = multi_task_model.bi_lstm.bias_hh_l0.data.clone()\n",
    "\n",
    "        single_task_model.bi_lstm.weight_ih_l1.data = multi_task_model.bi_lstm.weight_ih_l1.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l1.data = multi_task_model.bi_lstm.weight_hh_l1.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l1.data = multi_task_model.bi_lstm.bias_ih_l1.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l1.data = multi_task_model.bi_lstm.bias_hh_l1.data.clone()\n",
    "\n",
    "        #Tower\n",
    "        single_task_model.task_fc0[0].weight.data = multi_task_model.task_fc0[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc0[0].bias.data = multi_task_model.task_fc0[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_model.task_fc1[0].weight.data = multi_task_model.task_fc1[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc1[0].bias.data = multi_task_model.task_fc1[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_models[task_name] = single_task_model\n",
    "    return single_task_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a371a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    experiment_time\n",
    "    max_epoch\n",
    "    learning_rate\n",
    "    input_dim\n",
    "    task_name_list\n",
    "    train_loader_dict\n",
    "    val_dataset_dict\n",
    "    test_dataset_dict\n",
    "    device\n",
    "    is_show\n",
    "\n",
    "Output:\n",
    "    df_grade\n",
    "    stl_model_dict\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_and_test_model(experiment_time, max_epoch, learning_rate, input_dim, task_name_list, train_loader_dict, val_dataset_dict, test_dataset_dict, device,is_show = True):\n",
    "    df_grade = pd.DataFrame(columns=['time', 'task', 'auroc', 'acc', 'f1', 'pre', 'recall', 'brier_score', 'loss'])\n",
    "    best_model_params = {}\n",
    "    global_best_AUC = 0\n",
    "    global_best_loss = 10000\n",
    "    best_model_dict = {} \n",
    "    \n",
    "    count = 1\n",
    "    local_indicator = 'auroc'\n",
    "    global_indicator = 'loss'\n",
    "    \n",
    "\n",
    "    \n",
    "    for time in range(experiment_time):\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        auc_list = []    \n",
    "        local_best_AUC = 0\n",
    "        local_best_loss = 10000\n",
    "        local_best_model_dict = {} \n",
    "        patience_counter = 0\n",
    "        \n",
    "        model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "        for epoch in tqdm(range(max_epoch)):\n",
    "            if is_show:\n",
    "                print(f'Time:{time+1}/{experiment_time} - Epoch:{epoch+1}/{max_epoch}...')\n",
    "                \n",
    "            train_loss = train(model, train_loader_dict, loss_func, optimizer)\n",
    "            val_score_dict, result = test(model, val_dataset_dict, loss_func, is_show= is_show)\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "            val_loss_list.append(result['total_loss'])\n",
    "            auc_list.append(result['total_auc'])\n",
    "            \n",
    "            ########################################################################################################################\n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in local_best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = model.state_dict().copy()\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    local_best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            local_best_model_dict[task_name]['id'] = count                           \n",
    "                    else:\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            local_best_model_dict[task_name]['id'] = count\n",
    "                    \n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = model.state_dict().copy()\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "                    else:\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "            count+=1\n",
    "\n",
    "            ########################################################################################################################\n",
    "            \"\"\" Early stop \"\"\"\n",
    "            if global_indicator == 'loss':\n",
    "                if result['total_loss'] < local_best_loss:\n",
    "                    local_best_loss = result['total_loss']\n",
    "                    if local_best_loss < global_best_loss:\n",
    "                        global_best_loss = local_best_loss\n",
    "                        best_model_params = model.state_dict().copy() \n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1     \n",
    "            else:\n",
    "                if result['total_auc'] > local_best_AUC:\n",
    "                    local_best_AUC = result['total_auc']\n",
    "                    if local_best_AUC > global_best_AUC:\n",
    "                        global_best_AUC = local_best_AUC\n",
    "                        best_model_params = model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1 \n",
    "            \n",
    "            global limit_early_stop_count\n",
    "            if patience_counter >= limit_early_stop_count:\n",
    "                break\n",
    "        \n",
    "        #is_show = True\n",
    "        empty_model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        test_score_dict, result = test2(local_best_model_dict, empty_model, test_dataset_dict, loss_func, is_show = is_show)\n",
    "        ########################################################################################################################\n",
    "        #input()\n",
    "        for task_name in task_name_list:\n",
    "            test_score_dict[task_name]['time'] = time + 1\n",
    "            df_grade = pd.concat([df_grade, pd.DataFrame.from_records([test_score_dict[task_name]])])\n",
    "        print(df_grade)\n",
    "        print('----------finished----------')\n",
    "    \n",
    "    \n",
    "    global_stl_model_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        model.load_state_dict(best_model_dict[task_name]['model'])\n",
    "        local_stl_model_dict = MTL_to_STL(model)\n",
    "        global_stl_model_dict[task_name] = local_stl_model_dict[task_name]\n",
    "    \n",
    "    return df_grade, global_stl_model_dict, best_model_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Input:\n",
    "    shap_value: flatten的結果,[sample,feature]\n",
    "    feature_name_list: flatten的feature list\n",
    "Output:\n",
    "    feature_important \n",
    "    sum_per_feature \n",
    "    ++\n",
    "\"\"\"\n",
    "def calculate_feature_important(shap_value,feature_name_list,seq_day, n_temporal, n_static):\n",
    "    #特徵+入時序 ex. PEEP_D1 PEEP_D2 ... \n",
    "    full_feature_name_list = []\n",
    "    for i in range(len(feature_name_list)):\n",
    "        name = feature_name_list[i]\n",
    "        #static\n",
    "        if i >=n_temporal:\n",
    "            full_feature_name_list.append(f'{name}')\n",
    "        #temporal\n",
    "        else:\n",
    "            for day in range(seq_day):\n",
    "                full_feature_name_list.append(f'{name}_D{day+1}')\n",
    "                         \n",
    "    abs_shap_value = np.abs(shap_value)\n",
    "    sum_per_feature = np.sum(abs_shap_value, axis=0) #feature,value\n",
    "    \n",
    "    assert len(full_feature_name_list) == sum_per_feature.shape[0], f'{len(full_feature_name_list)}//{sum_per_feature.shape[0]}'\n",
    "    \n",
    "    #未排序\n",
    "    feature_important_dict = dict(zip(full_feature_name_list, sum_per_feature))\n",
    "    \n",
    "    sorted_feature_indices = np.argsort(sum_per_feature)[::-1] #[::-1]是reversed\n",
    "    sorted_feature_names = [full_feature_name_list[i] for i in sorted_feature_indices]\n",
    "    return sorted_feature_names, sum_per_feature, feature_important_dict\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    model  \n",
    "    train_X (np)\n",
    "    test_X  (np)\n",
    "    test_X_original (np)\n",
    "    feature_name_list (list)\n",
    "    task_name (string)\n",
    "    use_mini_sample (是否少量資料計算shap)\n",
    "    n_sample, (shap參數)\n",
    "    n_temporal,\n",
    "    n_static \n",
    "    \n",
    "Output:\n",
    "    shap_value\n",
    "    shap_data\n",
    "    (flatten的結果)\n",
    "\"\"\"\n",
    "#####################################################################################\n",
    "def get_model_shap(model,data_X_train,data_X_test,data_X_test_original,feature_name_list,task_name,use_mini_sample = True,n_sample = 100,n_temporal = 103,n_static = 53):\n",
    "    max_sample = 1000\n",
    "    seq_day = data_X_train.shape[1]\n",
    "    feature_count = data_X_train.shape[2]\n",
    "    \n",
    "    if use_mini_sample:\n",
    "        background_data = torch.from_numpy(data_X_train[:max_sample]).float().to(device)\n",
    "        shap_data = torch.from_numpy(data_X_test[:max_sample]).float().to(device)\n",
    "        shap_data_original = torch.from_numpy(data_X_test_original[:max_sample]).float().to(device)\n",
    "    else:\n",
    "        background_data = torch.from_numpy(data_X_train[:]).float().to(device)\n",
    "        shap_data = torch.from_numpy(data_X_test[:]).float().to(device)\n",
    "        shap_data_original = torch.from_numpy(data_X_test_original[:]).float().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    explainer = shap.GradientExplainer(model, background_data)\n",
    "    \n",
    "    shap_values = explainer.shap_values(shap_data,nsamples=n_sample)\n",
    "    shap_values = np.array(shap_values)\n",
    "    \n",
    "    shap_value_flatten = np.zeros((len(shap_data),seq_day*n_temporal + n_static))\n",
    "    shap_data_flatten = np.zeros((len(shap_data),seq_day*n_temporal + n_static))\n",
    "    \n",
    "    for i in range(0,len(shap_data)):\n",
    "        count=0\n",
    "        for j in range(feature_count):\n",
    "            #static\n",
    "            if j >= n_temporal:\n",
    "                for k in range(seq_day):\n",
    "                    shap_value_flatten[i][count]=shap_values[i][k][j]  \n",
    "                    shap_data_flatten[i][count]=shap_data_original[i][k][j]  \n",
    "                shap_value_flatten[i][count]/= seq_day\n",
    "                shap_data_flatten[i][count]/= seq_day\n",
    "                count += 1\n",
    "            #temporal\n",
    "            else:\n",
    "                for k in range(seq_day):\n",
    "                    shap_value_flatten[i][count]=shap_values[i][k][j]  \n",
    "                    shap_data_flatten[i][count]=shap_data_original[i][k][j]  \n",
    "                    count += 1\n",
    "            \n",
    "    feature_important, _, feature_important_dict = calculate_feature_important(shap_value_flatten, feature_name_list, seq_day, n_temporal, n_static)\n",
    "    return feature_important, shap_value_flatten, shap_data_flatten, feature_important_dict\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    shap_value_flatten (sample,feature_flatten)\n",
    "    shap_data_flatten (sample,feature_flatten)\n",
    "    max_display \n",
    "\"\"\"\n",
    "def show_shap(shap_value_flatten, shap_data_flatten,feature_name_list, max_display = 20,task_name = '',plot_type = 'dot'):\n",
    "    fig = shap.summary_plot(shap_value_flatten,shap_data_flatten,feature_names=feature_name_list,plot_type = plot_type, show=False,max_display = max_display)\n",
    "    #plt.title(f\"***Task:{task_name}***\")\n",
    "    #plt.xticks(fontsize=20, fontweight='bold', fontfamily='Arial')\n",
    "    #plt.yticks(fontsize=20, fontweight='bold', fontfamily='Arial')\n",
    "    #plt.xlabel('SHAP Value',fontsize=24, fontweight='bold', fontfamily='Arial')\n",
    "    #plt.ylabel(fontsize=26, fontweight='bold', fontfamily='Arial')\n",
    "    \n",
    "    #ax = plt.gca()  # 获取当前图形的轴\n",
    "    #plt.savefig(f'./解釋用模型/解釋結果/SHAP_{plot_type}.tif', bbox_inches = 'tight', dpi=300)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31150f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8e612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cbcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select_feature_list = []\n",
    "top_percent = 0.9\n",
    "remove_time_count = 0\n",
    "full_result_dict = {}\n",
    "select_feature_list = []\n",
    "\n",
    "#20241002\n",
    "# n_temporal = 128\n",
    "# n_static = 143\n",
    "\n",
    "#20250507\n",
    "n_temporal = 126\n",
    "n_static = 145\n",
    "\n",
    "\"\"\"\n",
    "read data\n",
    "\"\"\"\n",
    "train_dataset_dict,train_loader_dict,feature_name_list,_ = read_data(task_name_list,'','train',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "val_dataset_dict,val_loader_dict,_,_ = read_data(task_name_list,'','validation',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "test_dataset_dict,test_loader_dict,_,_ = read_data(task_name_list,'','test',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "if len(select_feature_list)!=0:\n",
    "    feature_name_list = select_feature_list\n",
    "input_dim = n_temporal + n_static\n",
    "print(f'==> input_dim: {input_dim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "n_temporal = 130\n",
    "n_static = 143\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f06fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++\n",
    "stl_model_dict = {}\n",
    "\n",
    "for task_name in task_name_list:\n",
    "    model = RNN_MTL(input_dim,[task_name]).to(device)\n",
    "    route = ''\n",
    "    if len(task_name_list) == 1:\n",
    "        #route = f'model_parm/STL/{task_name}'\n",
    "        \n",
    "        route = f'C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/STL/{task_name}'\n",
    "    else:\n",
    "        #route = 'model_parm/MTL/'+('+'.join(task_name_list)) \n",
    "        route = 'C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/MTL/'+('+'.join(task_name_list)) \n",
    "         \n",
    "    model.load_state_dict(torch.load(f'{route}/{task_name}_1'))    \n",
    "    model.n_temporal = n_temporal\n",
    "    model.n_static = n_static\n",
    "    stl_model_dict[task_name] = model\n",
    "    \n",
    "    task_dict = {}\n",
    "    task_dict[task_name] = test_dataset_dict[task_name]\n",
    "    result,_ = test(model, task_dict, loss_func, is_show = False)\n",
    "    print(result[task_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2077095",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dict['DNR'].inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94521e1a",
   "metadata": {},
   "source": [
    "# 以下開始Error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "分析的model\n",
    "\"\"\"\n",
    "model = RNN_MTL(input_dim,['DNR']).to(device)\n",
    "model.load_state_dict(torch.load(f'{route}/{task_name}_1'))    \n",
    "model.n_temporal = n_temporal\n",
    "model.n_static = n_static\n",
    "result,_ = test(model, {'DNR':test_dataset_dict['DNR']}, loss_func, is_show = False)\n",
    "print(result['DNR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "分析的model\n",
    "\"\"\"\n",
    "# model = RNN_MTL(input_dim,task_name_list).to(device)\n",
    "# model.load_state_dict(torch.load(f'{route}/{task_name}_1'))    \n",
    "# model.n_temporal = n_temporal\n",
    "# model.n_static = n_static\n",
    "\n",
    "# model.load_state_dict(torch.load(f'{route}/{task_name}_1'))    \n",
    "#     model.n_temporal = n_temporal\n",
    "#     model.n_static = n_static\n",
    "#     stl_model_dict[task_name] = model\n",
    "    \n",
    "#     task_dict = {}\n",
    "#     task_dict[task_name] = test_dataset_dict[task_name]\n",
    "#     result,_ = test(model, task_dict, loss_func, is_show = False)\n",
    "#     print(result[task_name])\n",
    "\n",
    "model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "model.load_state_dict(torch.load(f'{route}/{task_name}_1'), strict=False)\n",
    "model.n_temporal = n_temporal\n",
    "model.n_static = n_static\n",
    "task_dict[task_name] = test_dataset_dict[task_name]\n",
    "result,_ = test(model, task_dict, loss_func, is_show = False)\n",
    "print(result[task_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028452a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.common.constants import ShapValuesOutput, ModelTask\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Explainer Used: Mimic Explainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LinearExplainableModel\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from raiwidgets import ErrorAnalysisDashboard\n",
    "from interpret_community.common.constants import ShapValuesOutput, ModelTask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e94615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义预测函数和预测概率函数\n",
    "def predict_func(model, X):\n",
    "    task_name = model.task_name_list[0]\n",
    "    return model.predict(X)[task_name]\n",
    "\n",
    "def predict_proba_func(model, X):\n",
    "    task_name = model.task_name_list[0]\n",
    "    result = model.predict_proba(X)[task_name]\n",
    "    result = result.cpu().detach().numpy()\n",
    "    return result \n",
    "\n",
    "def create_model_pipeline(model):\n",
    "    # 載入標準化模型\n",
    "    #scaler_model = joblib.load('scaler_model.joblib')\n",
    "    # 將模型包裝在Pipeline中，依序進行轉換和預測\n",
    "    model_pipeline = Pipeline([\n",
    "        #('scaler', scaler_model),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # 添加自定义的 predict 和 predict_proba 方法\n",
    "    model_pipeline.predict = lambda X: predict_func(model_pipeline.named_steps['model'], X)\n",
    "    model_pipeline.predict_proba = lambda X: predict_proba_func(model_pipeline.named_steps['model'], X)\n",
    "\n",
    "    return model_pipeline\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import joblib\n",
    "\n",
    "# def predict_func(model, X, scaler_model=None):\n",
    "#     task_name = model.task_name_list[0]\n",
    "#     predictions = model.predict(X)[task_name]\n",
    "    \n",
    "#     # 還原標準化，如果提供了 scaler_model\n",
    "#     if scaler_model is not None:\n",
    "#         predictions = scaler_model.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    \n",
    "#     return predictions\n",
    "# def create_model_pipeline(model):\n",
    "#     # 載入標準化模型\n",
    "#     scaler_model = joblib.load(f'C:/Users/USER/M1326168/MIMIC/DNR/20240904/data/scaler_model.joblib')\n",
    "    \n",
    "#     # 將模型包裝在Pipeline中，依序進行轉換和預測\n",
    "#     model_pipeline = Pipeline([\n",
    "#         ('scaler', scaler_model),\n",
    "#         ('model', model)\n",
    "#     ])\n",
    "\n",
    "#     # 添加自定义的 predict 和 predict_proba 方法\n",
    "#     model_pipeline.predict = lambda X: predict_func(model_pipeline.named_steps['model'], X, scaler_model=scaler_model)\n",
    "#     model_pipeline.predict_proba = lambda X: predict_proba_func(model_pipeline.named_steps['model'], X)\n",
    "\n",
    "#     return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = create_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf9d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_original = train_dataset_dict['DNR'].inputs.numpy()\n",
    "X_train_original = train_dataset_dict[task_name].inputs.numpy()\n",
    "X_train_original = np.squeeze(X_train_original)\n",
    "\n",
    "#X_test_original_full = test_dataset_dict['DNR'].inputs.numpy()\n",
    "X_test_original_full = test_dataset_dict[task_name].inputs.numpy()\n",
    "\n",
    "X_test_original_full = np.squeeze(X_test_original_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_original_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = X_test_original_full.shape[0]\n",
    "#y_test_full = test_dataset_dict['DNR'].labels.numpy()\n",
    "y_test_full = test_dataset_dict[task_name].labels.numpy()\n",
    "X_test_original = X_test_original_full[:int(sample_count*0.9),:]\n",
    "y_test = y_test_full[:int(sample_count*0.9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de412ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41186591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徵名稱 + 時序\n",
    "full_feature_name_list = []\n",
    "seq_day = 3\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(feature_name_list)):\n",
    "    name = feature_name_list[i]\n",
    "    for day in range(seq_day):\n",
    "        full_feature_name_list.append(f'{name}_D{day+1}')\n",
    "\"\"\"\n",
    "for day in range(seq_day):\n",
    "    for i in range(len(feature_name_list)):\n",
    "        name = feature_name_list[i]\n",
    "        #full_feature_name_list.append(f'{name}_day{day+1}')\n",
    "        full_feature_name_list.append(f'{name}_day-{day-1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455368c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task = ModelTask.Classification\n",
    "explainer = MimicExplainer(model_pipeline, X_train_original, LGBMExplainableModel,\n",
    "                           augment_data=True, max_num_of_augmentations=10,\n",
    "                           features=full_feature_name_list, classes=[0,1], model_task=model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_explanation = explainer.explain_global(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b25906",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_pipeline = create_model_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7eb4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run error analysis on the full dataset with subsampled explanation data on 5k rows\n",
    "# Note in this case we need to provide the true_y_dataset parameter matching the\n",
    "# original full dataset\n",
    "ErrorAnalysisDashboard(global_explanation, dashboard_pipeline, dataset=X_test_original_full,\n",
    "                       true_y=y_test, categorical_features = [],\n",
    "                       true_y_dataset=y_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_scaled = y_test_full[0].reshape(1, -1)\n",
    "print(sample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2490502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#scaler = joblib.load('./data/scaler_model.joblib')\n",
    "\n",
    "scaler = joblib.load('C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/scaler_model.joblib')\n",
    "#df_feature = pd.read_csv(f'./full_feature/{data_date}_columns_name.csv')\n",
    "df_feature = pd.read_csv(f'C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/full_feature_name.csv')\n",
    "\n",
    "#不含時間\n",
    "feature_name_list = df_feature.columns.tolist()\n",
    "#含時間\n",
    "select_feature_list = full_feature_name_list\n",
    "\n",
    "\n",
    "print(f'含時序特徵數:{len(select_feature_list)}\\n不含時序特徵數:{len(feature_name_list)}')\n",
    "\n",
    "feature_count = len(feature_name_list)\n",
    "feature_count_with_time = len(select_feature_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f11d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_feature(select_feature_list):\n",
    "    for i in range(len(select_feature_list)):\n",
    "        print(f'[{i}]...{select_feature_list[i]}')\n",
    "        \n",
    "    select_id = int(input('feature id = '))\n",
    "    value = float(input('value = '))\n",
    "    \n",
    "    return select_feature_list[select_id], value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2ac9d",
   "metadata": {},
   "source": [
    "# 手動選擇節點特徵與值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name1, value1 = choose_feature(feature_name_list)\n",
    "\n",
    "day = int(input('day = '))\n",
    "feature_name1_with_time = feature_name1 + f\"_D{day}\"\n",
    "\n",
    "\n",
    "# index_of_feature1_full = feature_name_list_notime.index(feature_name1)\n",
    "# index_of_feature1_lite = select_feature_list.index(feature_name1_with_time)\n",
    "\n",
    "index_of_feature1_full = feature_name_list.index(feature_name1)\n",
    "index_of_feature1_lite = select_feature_list.index(feature_name1_with_time)\n",
    "\n",
    "\n",
    "#建構虛擬資料\n",
    "data = np.full(feature_count, value1)\n",
    "data = data.reshape(1,feature_count)\n",
    "original_value = scaler.inverse_transform(data)[0,index_of_feature1_full]\n",
    "#original_value = data\n",
    "\n",
    "print(f'特徵:{feature_name1} = {value1} 的原始值為: {original_value}')\n",
    "str1 = f'特徵:{feature_name1} = {value1} 的原始值為: {original_value}'\n",
    "\n",
    "\n",
    "#154 0.35 1\n",
    "#155 0.35\n",
    "\n",
    "#apdiii = 131 0.30 1\n",
    "#PO2 = 34\n",
    "#first_day_Platelets x1000 = 214\n",
    "\n",
    "#first_day_PT-INR = 226 0.1 1\n",
    "#first_day_Sodium = 218 0.7 1\n",
    "#first_day_Nutrition_Enteral_value = 233 0.39 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0323f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name2, value2 = choose_feature(feature_name_list)\n",
    "\n",
    "day = int(input('day = '))\n",
    "feature_name2_with_time = feature_name2 + f\"_D{day}\"\n",
    "\n",
    "\n",
    "# index_of_feature2_full = feature_name_list_notime.index(feature_name2)\n",
    "# index_of_feature2_lite = select_feature_list.index(feature_name2_with_time)\n",
    "\n",
    "index_of_feature2_full = feature_name_list.index(feature_name2)\n",
    "index_of_feature2_lite = select_feature_list.index(feature_name2_with_time)\n",
    "\n",
    "#建構虛擬資料\n",
    "data = np.full(feature_count, value2)\n",
    "data = data.reshape(1,feature_count)\n",
    "original_value = scaler.inverse_transform(data)[0,index_of_feature2_full]\n",
    "#original_value = data\n",
    "\n",
    "print(f'特徵:{feature_name2} = {value2} 的原始值為: {original_value}')\n",
    "str2 = f'特徵:{feature_name2} = {value2} 的原始值為: {original_value}'\n",
    "\n",
    "#220 0.19 1\n",
    "\n",
    "#PO2 = 34 0.08 3\n",
    "#first_day_PH =  219 0.82 1\n",
    "\n",
    "#first_day_Systemic Mean = 185 0.05 1\n",
    "\n",
    "#first_day_Sodium = 218 0.7 1\n",
    "\n",
    "#first_day_Sodium = 218 0.7 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name3, value3 = choose_feature(feature_name_list)\n",
    "\n",
    "day = int(input('day = '))\n",
    "feature_name3_with_time = feature_name3 + f\"_D{day}\"\n",
    "\n",
    "\n",
    "# index_of_feature3_full = feature_name_list_notime.index(feature_name3)\n",
    "# index_of_feature3_lite = select_feature_list.index(feature_name3_with_time)\n",
    "\n",
    "index_of_feature3_full = feature_name_list.index(feature_name3)\n",
    "index_of_feature3_lite = select_feature_list.index(feature_name3_with_time)\n",
    "\n",
    "#建構虛擬資料\n",
    "data = np.full(feature_count, value1)\n",
    "data = data.reshape(1,feature_count)\n",
    "original_value = scaler.inverse_transform(data)[0,index_of_feature3_full]\n",
    "#original_value = data\n",
    "\n",
    "print(f'特徵:{feature_name3} = {value3} 的原始值為: {original_value}')\n",
    "str3 = f'特徵:{feature_name3} = {value3} 的原始值為: {original_value}'\n",
    "\n",
    "#236 0.13 1\n",
    "#first_day_Platelets x1000 = 214 0.36 1\n",
    "#first_day_Tidal = 209 0.57 1\n",
    "#Cortisol = 176 0.5 1\n",
    "\n",
    "#first_day_Hgb = 211 0.13 1\n",
    "\n",
    "#first_day_Nutrition_Enteral_value = 233 0.39 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str1)\n",
    "print(str2)\n",
    "print(str3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08df2b",
   "metadata": {},
   "source": [
    "# Lite資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(MyDataset(X_scalar,X_original,Y))\n",
    "# ErrorAnalysisDashboard(global_explanation, dashboard_pipeline, dataset=X_test_original_full,\n",
    "#                        true_y=y_test, categorical_features = [],\n",
    "#                        true_y_dataset=y_test_full)\n",
    "\n",
    "#         dataset_dict[task_name] = MyDataset(X_scalar,X_original,Y)\n",
    "#         original_data_dict['X_scalar'] = X_scalar\n",
    "#         original_data_dict['X'] = X_original\n",
    "        \n",
    "#         original_data_dict['Y'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d576486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test\n",
    "# \"\"\" 讀lite資料集 \"\"\"\n",
    "# _, _, _, data = read_data(\n",
    "#     task_name_list,    # 多任務清單，例如 ['DNR','dod_30day','Vasopressor']\n",
    "#     '',                # 資料路徑改為空字串，代表讀預設的 lite 版本\n",
    "#     'test',            # 指定載入的資料切分(split)為 test\n",
    "#     select_feature_list,  # 只選取你先前定義好的特徵清單\n",
    "#     batch_size = batch_size,\n",
    "#     use_upsample = use_upsample\n",
    "# )\n",
    "\n",
    "# # 從回傳的 data dict 中取出：\n",
    "# X_scalar   = data['X_scalar']   # 經過 scaler 正規化後的特徵矩陣（float tensor）\n",
    "# X_original = data['X']          # 原始值矩陣（inverse_transform 後的結果）\n",
    "# Y          = data['Y']          # 對應的標籤向量\n",
    "\n",
    "# # 封裝成 PyTorch Dataset，方便後續 DataLoader 使用\n",
    "# dataset_dict = {}\n",
    "# dataset_dict[task_name] = MyDataset(X_scalar, X_original, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test_dataset_dict['DNR'].inputs shape: {test_dataset_dict['DNR'].inputs.shape}\")\n",
    "# print(f\"X_scalar shape: {X_scalar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2580c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查資料分割比例\n",
    "# sample_count = X_scalar.shape[0]\n",
    "# test_count = test_dataset_dict['DNR'].inputs.shape[0]\n",
    "# print(f\"總樣本數: {sample_count}, 測試集樣本數: {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacea5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_, _, _, data = read_data(task_name_list, '', 'test', select_feature_list, batch_size=batch_size, use_upsample=use_upsample)\n",
    "\n",
    "_, _, _, data = read_data([task_name], '', 'test', select_feature_list, batch_size=batch_size, use_upsample=use_upsample)\n",
    "\n",
    "# 使用正确的键名\n",
    "X_scalar = data['X_scalar']\n",
    "X_original = data['X']\n",
    "Y = data['Y']\n",
    "\n",
    "print(f\"X_scalar shape: {X_scalar.shape}\")\n",
    "print(f\"X_original shape: {X_original.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf131dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_samples_np(data, feature_index, threshold, condition_type):\n",
    "    \"\"\"\n",
    "    Remove samples based on a specified condition on a specific feature.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): Input data with shape [sample, 1, feature].\n",
    "    - feature_index (int): Index of the feature.\n",
    "    - threshold (float): Threshold value for the condition.\n",
    "    - condition_type (str): Type of condition ('type1' for '<' or 'type2' for '>=').\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Updated data after removing samples.\n",
    "    \"\"\"\n",
    "    if condition_type == 'type1':\n",
    "        indices_to_remove = np.squeeze(np.argwhere(data[:,  feature_index] < threshold))\n",
    "    elif condition_type == 'type2':\n",
    "        indices_to_remove = np.squeeze(np.argwhere(data[:,  feature_index] <= threshold))\n",
    "    elif condition_type == 'type3':\n",
    "        indices_to_remove = np.squeeze(np.argwhere(data[:,  feature_index] > threshold))\n",
    "    elif condition_type == 'type4':\n",
    "        indices_to_remove = np.squeeze(np.argwhere(data[:,  feature_index] >= threshold))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid condition_type. Use 'type1' for '<' or 'type2' for '>='.\")\n",
    "    \n",
    "    return indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc61d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "手動選擇type\n",
    "\"\"\"\n",
    "#type1: <\n",
    "#type2: <=\n",
    "#type3: >\n",
    "#type4: >=\n",
    "indices_to_remove1 = remove_samples_np(X_scalar,index_of_feature1_lite,value1,'type3')\n",
    "indices_to_remove2 = remove_samples_np(X_scalar,index_of_feature2_lite,value2,'type3')\n",
    "indices_to_remove3 = remove_samples_np(X_scalar,index_of_feature3_lite,value3,'type2')\n",
    "\n",
    "#common_indices = np.intersect1d(np.intersect1d(indices_to_remove1, indices_to_remove2), indices_to_remove3)\n",
    "\n",
    "common_indices = np.intersect1d(indices_to_remove1, indices_to_remove2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_original.shape[0])\n",
    "print(X_scalar.shape[0])\n",
    "print(Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fad8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'總樣本數(刪除前):{X_scalar.shape[0]}')\n",
    "print(f'條件1樣本數:{len(indices_to_remove1)}')\n",
    "print(f'條件2樣本數:{len(indices_to_remove2)}')\n",
    "print(f'條件3樣本數:{len(indices_to_remove3)}')\n",
    "print(f'符合條件的索引數:{len(common_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 刪除的樣本 \"\"\"\n",
    "X_scalar_remove = X_scalar[common_indices].copy()\n",
    "X_original_remove = X_original[common_indices].copy()\n",
    "Y_remove = Y[common_indices].copy()\n",
    "print(X_scalar_remove.shape)\n",
    "\n",
    "\"\"\" 保留的索引 \"\"\"\n",
    "full_indices = np.arange(0, Y.shape[0])\n",
    "# 找到在 full_indices 中不在 common_indices 中的索引\n",
    "keep_indices = np.array([i for i in full_indices if i not in common_indices])\n",
    "print(keep_indices.shape)\n",
    "\n",
    "\"\"\" 保留的樣本 \"\"\"\n",
    "X_scalar_keep = X_scalar[keep_indices].copy()\n",
    "X_original_keep = X_original[keep_indices].copy()\n",
    "Y_keep = Y[keep_indices].copy()\n",
    "\n",
    "print(f'刪除後 剩餘樣本數:{X_scalar_keep.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19147bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" 剩餘的樣本結果 \"\"\"\n",
    "dataset_dict_keep = {}\n",
    "dataset_dict_keep[task_name] = MyDataset(X_scalar_keep,X_original_keep,Y_keep)\n",
    "#原始分數\n",
    "print(\"*** 原始分數 ***\")\n",
    "#result_before_remove,_ = test(model, {'DNR':test_dataset_dict['DNR']}, loss_func, is_show = True)\n",
    "result_before_remove,_ = test(model, {'DNR':test_dataset_dict['DNR']}, loss_func, is_show = True)\n",
    "print(\"*** 新分數 ***\")\n",
    "#新分數\n",
    "result_after_remove,_ = test(model, dataset_dict_keep, loss_func, is_show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確保 task_name 是有效的\n",
    "task_name = 'DNR'  # 或從 task_name_list 中取得\n",
    "if task_name not in test_dataset_dict:\n",
    "    raise KeyError(f\"Task name '{task_name}' not found in test_dataset_dict.\")\n",
    "\n",
    "# 檢查資料形狀\n",
    "print(f\"X_scalar_keep shape: {X_scalar_keep.shape}\")\n",
    "print(f\"X_original_keep shape: {X_original_keep.shape}\")\n",
    "print(f\"Y_keep shape: {Y_keep.shape}\")\n",
    "if X_scalar_keep.shape[0] != X_original_keep.shape[0] or X_scalar_keep.shape[0] != Y_keep.shape[0]:\n",
    "    raise ValueError(\"Mismatch in the number of samples between X_scalar_keep, X_original_keep, and Y_keep.\")\n",
    "\n",
    "# 建立剩餘的樣本結果\n",
    "dataset_dict_keep = {}\n",
    "dataset_dict_keep[task_name] = MyDataset(X_scalar_keep, X_original_keep, Y_keep)\n",
    "\n",
    "# 原始分數\n",
    "print(\"*** 原始分數 ***\")\n",
    "result_before_remove, _ = test(model, {'DNR': test_dataset_dict['DNR']}, loss_func, is_show=True)\n",
    "\n",
    "# 新分數\n",
    "print(\"*** 新分數 ***\")\n",
    "result_after_remove, _ = test(model, dataset_dict_keep, loss_func, is_show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame([result_before_remove['DNR'], result_after_remove['DNR']], index=['before', 'after'])\n",
    "if input('Save? (y/n)') == 'y':\n",
    "    df_result.to_csv(f'C:/Users/USER/M1326168/MIMIC/DNR/20250507/error_analysis/實驗圖/error_analysis結果.csv')\n",
    "    print('儲存成功')\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 總樣本分布 \"\"\"\n",
    "arr = Y.copy()\n",
    "\n",
    "# 計算 0 和 1 的數量\n",
    "total_sample = Y.shape[0]\n",
    "count_zero = np.count_nonzero(arr == 0)\n",
    "count_one = np.count_nonzero(arr == 1)\n",
    "\n",
    "# 顯示結果\n",
    "print(f'總樣本: {total_sample}')\n",
    "print(f\"Number of 0: {count_zero} ({round(count_zero/total_sample*100,2)}%)\")\n",
    "print(f\"Number of 1: {count_one} ({round(count_one/total_sample*100,2)}%)\")\n",
    "print(\"**************************************\")\n",
    "\n",
    "\"\"\" 移除的樣本分布\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "# 假設 arr 是你的 NumPy 陣列\n",
    "arr = Y_remove.copy()\n",
    "\n",
    "# 計算 0 和 1 的數量\n",
    "total_remove_sample = Y_remove.shape[0]\n",
    "count_zero = np.count_nonzero(arr == 0)\n",
    "count_one = np.count_nonzero(arr == 1)\n",
    "\n",
    "# 顯示結果\n",
    "print(f'移除的樣本: {total_remove_sample}')\n",
    "print(f\"標籤為0: {count_zero} ({round(count_zero/total_remove_sample*100,2)}%)\")\n",
    "print(f\"標籤為1: {count_one} ({round(count_one/total_remove_sample*100,2)}%)\")\n",
    "print(\"**************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, file_name, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    繪製混淆矩陣\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): 實際類別標籤\n",
    "    y_pred (array-like): 預測類別標籤\n",
    "    classes (list): 類別標籤的名稱列表\n",
    "    title (str): 圖表標題\n",
    "    cmap (matplotlib colormap): 顏色映射\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # 計算混淆矩陣\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 將混淆矩陣轉換為 DataFrame\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    \n",
    "    new_order = ['Class 1','Class 0']\n",
    "    new_order_columns = ['Class 1','Class 0']\n",
    "    # 使用 reindex 方法重新指定行的順序\n",
    "    cm_df = cm_df.reindex(new_order)\n",
    "    cm_df = cm_df.reindex(columns=new_order_columns)\n",
    "    #cm_df = cm_df.reorder_levels(['Class 1', 'Class 0'], axis=1)\n",
    "    print(cm_df)\n",
    "    #input()\n",
    "    \n",
    "    \n",
    "    # 調整標籤的顯示順序\n",
    "    y_labels = list(reversed(classes))\n",
    "    x_labels = list(reversed(classes))\n",
    "    #y_labels = list(classes)\n",
    "    #x_labels = list(classes)\n",
    "\n",
    "    # 使用 Seaborn 繪製熱度圖\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # 設置 xticklabels 和 yticklabels 的字體屬性\n",
    "    font_properties = {'size': 20,  'family': 'Arial'}\n",
    "    title_font_properties = {'size': 20, 'weight': 'bold', 'family': 'Arial'}\n",
    "    \n",
    "    annot_kws = {'size': 20, 'weight': 'bold', 'family': 'Arial'}\n",
    "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=cmap, cbar=False,\n",
    "                xticklabels=x_labels, yticklabels=y_labels, annot_kws=annot_kws)\n",
    "    \n",
    "    plt.xticks(fontproperties='Arial', **font_properties)\n",
    "    plt.yticks(fontproperties='Arial', **font_properties)\n",
    "    \n",
    "    # 設置 x 軸標籤\n",
    "    plt.xlabel(title, fontproperties='Arial', **title_font_properties)\n",
    "    plt.ylabel(title, fontproperties='Arial', **title_font_properties)\n",
    "    \n",
    "    # 設置圖的標題\n",
    "    #plt.title('Confusion Matrix', fontsize=20)\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'./error_analysis/實驗圖/confusion_{file_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" [1]所有的樣本 的 預測狀況 \"\"\"\n",
    "correct_count_0 = 0\n",
    "correct_count_1 = 0\n",
    "count_zero = np.count_nonzero(Y == 0)\n",
    "count_one = np.count_nonzero(Y == 1)\n",
    "count = 0\n",
    "pred_y = model.predict(X_scalar)['DNR']\n",
    "\n",
    "for i in range(pred_y.shape[0]):\n",
    "    if Y[i] != pred_y[i]:\n",
    "        count+=1\n",
    "    if Y[i] == 1 and pred_y[i] == 1:\n",
    "        correct_count_1 += 1\n",
    "    if Y[i] == 0 and pred_y[i] == 0:\n",
    "        correct_count_0 += 1\n",
    " \n",
    "plot_confusion_matrix(Y,pred_y,['Class 0','Class 1'],'full')\n",
    "        \n",
    "print(f'總樣本 1的樣本數: {count_one}')\n",
    "print(f\"--預測正確: {correct_count_1} ({round(correct_count_1/count_one*100,2)}%)\")\n",
    "print(f\"--預測失敗: {count_one-correct_count_1} ({round((count_one-correct_count_1)/count_one*100,2)}%)\")\n",
    "\n",
    "print(\"************************************************\")\n",
    "\n",
    "print(f'總樣本 0的樣本數: {count_zero}')\n",
    "print(f\"--預測正確: {correct_count_0} ({round(correct_count_0/count_zero*100,2)}%)\")\n",
    "print(f\"--預測失敗: {count_zero-correct_count_0} ({round((count_zero-correct_count_0)/count_zero*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" [2]移除的樣本 的 預測狀況 \"\"\"\n",
    "count_zero = np.count_nonzero(Y_remove == 0)\n",
    "count_one = np.count_nonzero(Y_remove == 1)\n",
    "correct_count_0 = 0\n",
    "correct_count_1 = 0\n",
    "\n",
    "pred_y = model.predict(X_scalar_remove)['DNR']\n",
    "\n",
    "for i in range(pred_y.shape[0]):\n",
    "    if Y_remove[i] == 1 and pred_y[i] == 1:\n",
    "        correct_count_1 += 1\n",
    "    if Y_remove[i] == 0 and pred_y[i] == 0:\n",
    "        correct_count_0 += 1\n",
    "\n",
    "plot_confusion_matrix(Y_remove,pred_y,['Class 0','Class 1'],'remove_sample')\n",
    "\n",
    "        \n",
    "print(f'移除的樣本 1的樣本數: {count_one}')\n",
    "print(f\"--預測正確: {correct_count_1} ({round(correct_count_1/count_one*100,2)}%)\")\n",
    "print(f\"--預測失敗: {count_one-correct_count_1} ({round((count_one-correct_count_1)/count_one*100,2)}%)\")\n",
    "\n",
    "print(\"************************************************\")\n",
    "\n",
    "print(f'移除的樣本 0的樣本數: {count_zero}')\n",
    "print(f\"--預測正確: {correct_count_0} ({round(correct_count_0/count_zero*100,2)}%)\")\n",
    "print(f\"--預測失敗: {count_zero-correct_count_0} ({round((count_zero-correct_count_0)/count_zero*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" [3]剩餘的樣本 的 預測狀況 \"\"\"\n",
    "count_zero = np.count_nonzero(Y_keep == 0)\n",
    "count_one = np.count_nonzero(Y_keep == 1)\n",
    "correct_count_0 = 0\n",
    "correct_count_1 = 0\n",
    "\n",
    "pred_y = model.predict(X_scalar_keep)['DNR']\n",
    "\n",
    "for i in range(pred_y.shape[0]):\n",
    "    if Y_keep[i] == 1 and pred_y[i] == 1:\n",
    "        correct_count_1 += 1\n",
    "    if Y_keep[i] == 0 and pred_y[i] == 0:\n",
    "        correct_count_0 += 1\n",
    "\n",
    "plot_confusion_matrix(Y_keep,pred_y,['Class 0','Class 1'],'keep_sample')\n",
    "        \n",
    "print(f'保留的樣本 1的樣本數: {count_one}')\n",
    "print(f\"--預測正確: {correct_count_1} ({round(correct_count_1/count_one*100,2)}%)\")\n",
    "print(f\"--預測失敗: {count_one-correct_count_1} ({round((count_one-correct_count_1)/count_one*100,2)}%)\")\n",
    "\n",
    "print(\"************************************************\")\n",
    "\n",
    "print(f'保留的樣本 0的樣本數: {count_zero}')\n",
    "print(f\"--預測正確: {correct_count_0} ({round(correct_count_0/count_zero*100,2)}%)\")\n",
    "print(f\"--預測失敗: {count_zero-correct_count_0} ({round((count_zero-correct_count_0)/count_zero*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4089737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03223aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18f62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b601496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40260479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f28c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5feab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef82f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

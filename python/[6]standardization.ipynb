{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5ae729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd1b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_name_list = ['dod','dod_3day','dod_7day','dod_30day','dod_60day','dod_90day','DNR','Weaning_successful','SBT']\n",
    "task_name_list = ['dod','dod_3day','dod_7day','dod_30day','dod_60day','dod_90day','DNR','Weaning_successful','SBT','dialysis','InvasiveVent','Vasopressor']\n",
    "\n",
    "#task_name_list = ['dialysis','InvasiveVent','Vasopressor']\n",
    "#data_path = \"./data/sample\"\n",
    "\n",
    "data_path = \"C:/Users/USER/M1326168/MIMIC/DNR/20241002/data/sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a5f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dod ==> (9572, 3, 271)\n",
      "dod ==> (9572,)\n",
      "dod ==> (1197, 3, 271)\n",
      "dod ==> (1197,)\n",
      "dod ==> (1197, 3, 271)\n",
      "dod ==> (1197,)\n",
      "******************\n",
      "dod_3day ==> (11203, 3, 271)\n",
      "dod_3day ==> (11203,)\n",
      "dod_3day ==> (1400, 3, 271)\n",
      "dod_3day ==> (1400,)\n",
      "dod_3day ==> (1401, 3, 271)\n",
      "dod_3day ==> (1401,)\n",
      "******************\n",
      "dod_7day ==> (12617, 3, 271)\n",
      "dod_7day ==> (12617,)\n",
      "dod_7day ==> (1577, 3, 271)\n",
      "dod_7day ==> (1577,)\n",
      "dod_7day ==> (1578, 3, 271)\n",
      "dod_7day ==> (1578,)\n",
      "******************\n",
      "dod_30day ==> (15132, 3, 271)\n",
      "dod_30day ==> (15132,)\n",
      "dod_30day ==> (1891, 3, 271)\n",
      "dod_30day ==> (1891,)\n",
      "dod_30day ==> (1892, 3, 271)\n",
      "dod_30day ==> (1892,)\n",
      "******************\n",
      "dod_60day ==> (16236, 3, 271)\n",
      "dod_60day ==> (16236,)\n",
      "dod_60day ==> (2030, 3, 271)\n",
      "dod_60day ==> (2030,)\n",
      "dod_60day ==> (2030, 3, 271)\n",
      "dod_60day ==> (2030,)\n",
      "******************\n",
      "dod_90day ==> (16752, 3, 271)\n",
      "dod_90day ==> (16752,)\n",
      "dod_90day ==> (2094, 3, 271)\n",
      "dod_90day ==> (2094,)\n",
      "dod_90day ==> (2095, 3, 271)\n",
      "dod_90day ==> (2095,)\n",
      "******************\n",
      "DNR ==> (12236, 3, 271)\n",
      "DNR ==> (12236,)\n",
      "DNR ==> (1530, 3, 271)\n",
      "DNR ==> (1530,)\n",
      "DNR ==> (1530, 3, 271)\n",
      "DNR ==> (1530,)\n",
      "******************\n",
      "Weaning_successful ==> (3936, 3, 271)\n",
      "Weaning_successful ==> (3936,)\n",
      "Weaning_successful ==> (492, 3, 271)\n",
      "Weaning_successful ==> (492,)\n",
      "Weaning_successful ==> (492, 3, 271)\n",
      "Weaning_successful ==> (492,)\n",
      "******************\n",
      "SBT ==> (7032, 3, 271)\n",
      "SBT ==> (7032,)\n",
      "SBT ==> (879, 3, 271)\n",
      "SBT ==> (879,)\n",
      "SBT ==> (880, 3, 271)\n",
      "SBT ==> (880,)\n",
      "******************\n",
      "dialysis ==> (11526, 3, 271)\n",
      "dialysis ==> (11526,)\n",
      "dialysis ==> (1441, 3, 271)\n",
      "dialysis ==> (1441,)\n",
      "dialysis ==> (1441, 3, 271)\n",
      "dialysis ==> (1441,)\n",
      "******************\n",
      "InvasiveVent ==> (22047, 3, 271)\n",
      "InvasiveVent ==> (22047,)\n",
      "InvasiveVent ==> (2756, 3, 271)\n",
      "InvasiveVent ==> (2756,)\n",
      "InvasiveVent ==> (2756, 3, 271)\n",
      "InvasiveVent ==> (2756,)\n",
      "******************\n",
      "Vasopressor ==> (11377, 3, 271)\n",
      "Vasopressor ==> (11377,)\n",
      "Vasopressor ==> (1422, 3, 271)\n",
      "Vasopressor ==> (1422,)\n",
      "Vasopressor ==> (1423, 3, 271)\n",
      "Vasopressor ==> (1423,)\n",
      "******************\n"
     ]
    }
   ],
   "source": [
    "all_task_data = {}\n",
    "\n",
    "for task_name in task_name_list:\n",
    "    \n",
    "    # read\n",
    "    train_X = np.load(f\"{data_path}/train_X_{task_name}.npy\", allow_pickle=True)\n",
    "    train_Y = np.load(f\"{data_path}/train_Y_{task_name}.npy\", allow_pickle=True)\n",
    "    validation_X = np.load(f\"{data_path}/validation_X_{task_name}.npy\", allow_pickle=True)\n",
    "    validation_Y = np.load(f\"{data_path}/validation_Y_{task_name}.npy\", allow_pickle=True)\n",
    "    test_X = np.load(f\"{data_path}/test_X_{task_name}.npy\", allow_pickle=True)\n",
    "    test_Y = np.load(f\"{data_path}/test_Y_{task_name}.npy\", allow_pickle=True)\n",
    "    \n",
    "    print(f'{task_name} ==> {train_X.shape}')\n",
    "    print(f'{task_name} ==> {train_Y.shape}')\n",
    "    print(f'{task_name} ==> {validation_X.shape}')\n",
    "    print(f'{task_name} ==> {validation_Y.shape}')\n",
    "    print(f'{task_name} ==> {test_X.shape}')\n",
    "    print(f'{task_name} ==> {test_Y.shape}')\n",
    "    print(\"******************\")\n",
    "\n",
    "    \n",
    "    all_task_data[task_name] = {\n",
    "        'train': {'X': train_X[:,:], 'X_with_id': train_X, 'Y': train_Y},\n",
    "        'validation': {'X': validation_X[:,:], 'X_with_id': validation_X,'Y': validation_Y},\n",
    "        'test': {'X': test_X[:,:], 'X_with_id': test_X, 'Y': test_Y}\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91e55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561270, 271)\n"
     ]
    }
   ],
   "source": [
    "num_features = all_task_data[task_name]['train']['X'].shape[2]\n",
    "\n",
    "#串接所有sample input\n",
    "combined_X = np.empty((0,num_features))\n",
    "for task_name in task_name_list:\n",
    "    for data_type in ['train','validation','test']:\n",
    "        x = all_task_data[task_name][data_type]['X']\n",
    "        reshaped_x = np.reshape(x, (-1,x.shape[2]))  # (sample*seq, feature)\n",
    "        combined_X = np.vstack((combined_X, reshaped_x))\n",
    "\n",
    "print(combined_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72821514",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21427349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/USER/M1326168/MIMIC/DNR/20241002/data/scaler_model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "#計算轉換器\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)).fit(combined_X[:,:])\n",
    "\n",
    "#保存轉換器\n",
    "#joblib.dump(scaler, './data/scaler_model.joblib')\n",
    "\n",
    "joblib.dump(scaler, 'C:/Users/USER/M1326168/MIMIC/DNR/20241002/data/scaler_model.joblib')\n",
    "\n",
    "#讀取轉換器\n",
    "#scaler = joblib.load('./data/scaler_model.joblib')\n",
    "#input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ee7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, scaler):\n",
    "    sample_count = data.shape[0]\n",
    "    seq_count = data.shape[1]\n",
    "    #print(data.shape)\n",
    "    assert seq_count == 3\n",
    "    reshape_data = np.reshape(data, (-1,data.shape[2]))\n",
    "    scalar_data = scaler.transform(reshape_data)\n",
    "    return np.reshape(scalar_data, (sample_count, seq_count, data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bbc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name in task_name_list:\n",
    "    # 對訓練集進行縮放\n",
    "    all_task_data[task_name]['train']['scalar_X'] = scale_data(all_task_data[task_name]['train']['X'][:,:], scaler)\n",
    "    # 對驗證集進行縮放\n",
    "    all_task_data[task_name]['validation']['scalar_X'] = scale_data(all_task_data[task_name]['validation']['X'][:,:], scaler)\n",
    "    # 對測試集進行縮放\n",
    "    all_task_data[task_name]['test']['scalar_X'] = scale_data(all_task_data[task_name]['test']['X'][:,:], scaler)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5866ecb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:dod => 1:1048(8.76%)   0:10918(91.24%)\n",
      "Task:dod_3day => 1:3675(26.24%)   0:10329(73.76%)\n",
      "Task:dod_7day => 1:5872(37.23%)   0:9900(62.77%)\n",
      "Task:dod_30day => 1:9930(52.5%)   0:8985(47.5%)\n",
      "Task:dod_60day => 1:11681(57.55%)   0:8615(42.45%)\n",
      "Task:dod_90day => 1:12540(59.88%)   0:8401(40.12%)\n",
      "Task:DNR => 1:5119(33.47%)   0:10177(66.53%)\n",
      "Task:Weaning_successful => 1:2164(43.98%)   0:2756(56.02%)\n",
      "Task:SBT => 1:4748(54.01%)   0:4043(45.99%)\n",
      "Task:dialysis => 1:3668(25.46%)   0:10740(74.54%)\n",
      "Task:InvasiveVent => 1:19800(71.85%)   0:7759(28.15%)\n",
      "Task:Vasopressor => 1:3752(26.38%)   0:10470(73.62%)\n"
     ]
    }
   ],
   "source": [
    "for task_name in task_name_list:\n",
    "    train_y = all_task_data[task_name]['train']['Y']\n",
    "    val_y = all_task_data[task_name]['validation']['Y']\n",
    "    test_y = all_task_data[task_name]['test']['Y']\n",
    "    \n",
    "    total = np.concatenate((train_y, val_y), axis=0)\n",
    "    total = np.concatenate((total, test_y), axis=0)\n",
    "    one_count = np.count_nonzero(total == 1)\n",
    "    zero_count = np.count_nonzero(total == 0)\n",
    "    \n",
    "    print(f'Task:{task_name} => 1:{one_count}({round(one_count*100/(one_count+zero_count),2)}%)   0:{zero_count}({round(zero_count*100/(one_count+zero_count),2)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536a416",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3fb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_path = './data/sample_new/standard_data/'\n",
    "#save_path = './data/sample/standard_data/'\n",
    "\n",
    "save_path = 'C:/Users/USER/M1326168/MIMIC/DNR/20241002/data/sample/standard_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f3f4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for task_name in task_name_list: #[Weaning、SBT、 ...]\n",
    "    for data_type in all_task_data[task_name].keys():  #train、validation、test\n",
    "        for data_name in all_task_data[task_name][data_type].keys():  #X、Y、scalar_X\n",
    "            file_name = f'{data_type}_{data_name}_{task_name}'\n",
    "            folder = os.path.join(save_path, file_name)\n",
    "            data = all_task_data[task_name][data_type][data_name]\n",
    "            np.save(f'{folder}.npy', data)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945cc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

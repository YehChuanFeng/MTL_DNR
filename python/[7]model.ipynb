{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 1\n",
    "seq_len = 1\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "max_epoch = 50\n",
    "experiment_time = 10\n",
    "limit_early_stop_count = 5\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#DNR數據是否需要upsample\n",
    "use_upsample = True\n",
    "\n",
    "#['dod','dod_3day','dod_7day','dod_30day','dod_60day','dod_90day','DNR','Weaning_successful','SBT','dialysis']\n",
    "#task_name_list = ['DNR']\n",
    "#task_name_list = ['dod_7day']\n",
    "#task_name_list = ['dod_30day']\n",
    "#task_name_list = ['dialysis']\n",
    "#task_name_list = ['Weaning_successful']\n",
    "#task_name_list = ['SBT']\n",
    "#task_name_list = ['dod_3day']\n",
    "#task_name_list = ['dod_90day']\n",
    "#task_name_list = ['InvasiveVent']\n",
    "#task_name_list = ['Vasopressor']\n",
    "#task_name_list = ['dod_60day']\n",
    "\n",
    "#task_name_list = ['DNR','dod_30day']\n",
    "#task_name_list = ['DNR','dod_7day']\n",
    "#task_name_list = ['DNR','dialysis']\n",
    "#task_name_list = ['DNR','Vasopressor']\n",
    "#task_name_list = ['DNR','InvasiveVent']\n",
    "#task_name_list = ['DNR','SBT']\n",
    "#task_name_list = ['DNR','Weaning_successful']\n",
    "#task_name_list = ['DNR','dod_3day']\n",
    "#task_name_list = ['DNR','dod_90day']\n",
    "#task_name_list = ['DNR','dod_60day']\n",
    "\n",
    "task_name_list = ['DNR','dod_30day','Vasopressor']\n",
    "#task_name_list = ['DNR','dod_30day','InvasiveVent']\n",
    "#task_name_list = ['DNR','dod_30day','dialysis']\n",
    "#task_name_list = ['DNR','InvasiveVent','dialysis']\n",
    "#task_name_list = ['DNR','InvasiveVent','Vasopressor']\n",
    "#task_name_list = ['DNR','Vasopressor','dialysis']\n",
    "\n",
    "#task_name_list = ['DNR','dod_30day','Vasopressor','InvasiveVent']\n",
    "#task_name_list = ['DNR','dod_30day','Vasopressor','dialysis']\n",
    "#task_name_list = ['DNR','dod_30day','Vasopressor','InvasiveVent','dialysis']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "##++\n",
    "class RNN_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, task_name_list,window_size = 3, dropout_ratio=0.0):\n",
    "        super(RNN_MTL, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.relu = nn.ReLU()  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.task_name_list = task_name_list\n",
    "        self.num_tasks = len(task_name_list)\n",
    "        hidden_dim = [256, 64]\n",
    "        output_size = 1\n",
    "        # Bottom\n",
    "        self.bi_lstm = torch.nn.LSTM(input_dim, hidden_dim[0], num_layers=2, batch_first = True, bidirectional = True)\n",
    "        # Towers\n",
    "        self.task_fc0 = nn.ModuleList([nn.Linear(hidden_dim[0]*window_size*2, hidden_dim[1]) for _ in range(self.num_tasks)])\n",
    "        self.task_fc1 = nn.ModuleList([nn.Linear(hidden_dim[1], output_size) for _ in range(self.num_tasks)])\n",
    "    \n",
    "    #數據檢查\n",
    "    def data_check(self,x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        #if x.ndim == 3:\n",
    "        #    x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])  # Flatten \n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, use_dropout = False):\n",
    "        x = self.data_check(x)\n",
    "        h, _   = self.bi_lstm(x)\n",
    "        h = torch.nn.functional.relu(h)\n",
    "        if use_dropout:\n",
    "            h = nn.Dropout(p=0.5)(h)\n",
    "        h = torch.flatten(h, start_dim = 1)\n",
    "        \n",
    "        # Towers\n",
    "        task_out = {}\n",
    "        for task_index in range(self.num_tasks):\n",
    "            task_name = self.task_name_list[task_index]\n",
    "            hi = self.task_fc0[task_index](h)\n",
    "            hi = self.relu(hi)\n",
    "            hi = self.dropout(hi)\n",
    "            hi = self.task_fc1[task_index](hi)\n",
    "            hi = self.sigmoid(hi)\n",
    "            task_out[task_name] = hi    \n",
    "        \n",
    "        #STL => 不回傳dict\n",
    "        if len(self.task_name_list) == 1:\n",
    "            return task_out[self.task_name_list[0]]\n",
    "        #MTL => 回傳dict\n",
    "        else:\n",
    "            return task_out\n",
    "    \n",
    "    #eval. 固定回傳dict\n",
    "    def predict_prob(self, x , use_dropout = False):\n",
    "        prob_dict = self.forward(x,use_dropout)\n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        return prob_dict\n",
    "    \n",
    "    #將預測結果轉標籤\n",
    "    def predict(self, x, threshold = 0.5):\n",
    "        self.eval()\n",
    "        prob_dict = self.predict_prob(x)\n",
    "        pred_dict = {}\n",
    "        for key, value in prob_dict.items():\n",
    "            #tensor轉numpy\n",
    "            value = value.cpu().detach().numpy()\n",
    "            pred_class = [1 if x > threshold else 0 for x in value]\n",
    "            pred_dict[key] = np.array(pred_class) \n",
    "        return pred_dict\n",
    "    \n",
    "    #計算評估指標\n",
    "    def evaluate(self,X,label,task_name,criterion):\n",
    "        with torch.no_grad():\n",
    "            prob = self.predict_prob(X)[task_name].cpu().detach().numpy() #tensor=>numpy\n",
    "            pred = self.predict(X)[task_name] \n",
    "            score = compute_scores(label,pred,prob)\n",
    "            score['task'] = task_name\n",
    "            loss = criterion(torch.from_numpy(prob).to(device),torch.from_numpy(label).to(device)).item()\n",
    "            score['loss'] = loss/len(label)\n",
    "\n",
    "            #新增 'predictions' 到 score，讓 test2() 可以存取\n",
    "            score['predictions'] = pred\n",
    "\n",
    "            return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8537c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "計算評估指標\n",
    "y_true: label\n",
    "y_pred: 預測標籤\n",
    "y_prob: 預測機率\n",
    "\"\"\"\n",
    "def compute_scores(y_true, y_pred, y_prob):\n",
    "    scores = {}\n",
    "    try:\n",
    "        scores['task'] = 'Null'\n",
    "        scores['auroc'] = round(roc_auc_score(y_true, y_prob), 3)\n",
    "        scores['acc'] = round(accuracy_score(y_true, y_pred), 3)\n",
    "        scores['f1'] = round(f1_score(y_true, y_pred), 3)\n",
    "        scores['pre'] = round(precision_score(y_true, y_pred), 3)\n",
    "        scores['recall'] = round(recall_score(y_true, y_pred), 3)\n",
    "        scores['brier_score'] = round(brier_score_loss(y_true, y_prob), 3)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f86098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_dict, criterion, optimizer):\n",
    "    \n",
    "\n",
    "    train_loss = 0\n",
    "    task_name_list = list(loader_dict.keys())\n",
    "\n",
    "    \"\"\"\n",
    "    cycle_loader\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    cycle_loader_dict = {}\n",
    "    for task_name, dataloader in loader_dict.items():\n",
    "        current_length = len(dataloader)\n",
    "        max_length = max(max_length, current_length)\n",
    "        cycle_loader_dict[task_name] = itertools.cycle(dataloader)\n",
    "    model.train()\n",
    "    for i in range(max_length):\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = torch.zeros(1).to(device)\n",
    "        \n",
    "        for task_name in task_name_list:  \n",
    "            data = next(cycle_loader_dict[task_name])\n",
    "            x = data[0].to(device)\n",
    "            label = data[1].unsqueeze(1).to(device)\n",
    "            \n",
    "            prob = model.predict_prob(x,True)[task_name]\n",
    "            loss = criterion(prob, label)\n",
    "            total_loss += loss\n",
    "        \n",
    "        \n",
    "        total_loss.backward()\n",
    "        train_loss+=total_loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #資料用完的任務集重新循環\n",
    "        for task_name in task_name_list: \n",
    "            if (i + 1) % len(loader_dict[task_name]) == 0:\n",
    "                cycle_loader_dict[task_name] = itertools.cycle(loader_dict[task_name])\n",
    "    \n",
    "    #棄用\n",
    "    train_loss /= max_length* 256 * len(task_name_list)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    model\n",
    "    dict: Mydataset\n",
    "    loss_function\n",
    "Output:\n",
    "    score: dict + dict\n",
    "    result: dict => ['total_auc','total_loss']\n",
    "\"\"\"\n",
    "def test(model, dataset_dict, criterion, is_show = True ):\n",
    "    model.eval()\n",
    "\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    for task_name in task_name_list:  # 循環每個任務\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "        score[task_name] = model.evaluate(X,Y,task_name,criterion)\n",
    "        \n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "            \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "\n",
    "    return score,result\n",
    "\n",
    "\"\"\"\n",
    "local_best_model_dict: #dict{'task_name':{'model','performance(target_score)','id'}}\n",
    "model\n",
    "\"\"\"\n",
    "def test2(local_best_model_dict, dataset_dict, criterion, is_show = True):\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    \n",
    "    for task_name in task_name_list:\n",
    "        modelr = local_best_model_dict[task_name]['model']\n",
    "        modelr.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        score[task_name] = modelr.evaluate(X,Y,task_name,criterion)\n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "\n",
    "\n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "\n",
    "    \n",
    "    return score,result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d074fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, np_X_scalar,np_X_original, np_Y):\n",
    "        self.inputs = torch.from_numpy(np_X_scalar).float()\n",
    "        self.inputs_original = torch.from_numpy(np_X_original).float()\n",
    "        self.labels = torch.from_numpy(np_Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, _input, target):\n",
    "        pt = _input\n",
    "        alpha = self.alpha\n",
    "        loss = - alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1 - alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "def check_label_distribution (data_Y):\n",
    "    count_1 = np.count_nonzero(data_Y == 1)\n",
    "    count_0 = np.count_nonzero(data_Y == 0)\n",
    "    count_others = np.count_nonzero((data_Y != 1) & (data_Y != 0))\n",
    "    ratio_1 = round(count_1/len(data_Y)*100,2)\n",
    "    ratio_0 = round(count_0/len(data_Y)*100,2)\n",
    "    ratio_others = round(count_others/len(data_Y)*100,2)\n",
    "    print(f'Distribution: 1=>{count_1}({ratio_1}%),  0=>{count_0}({ratio_0}%),  others=>{count_others}({ratio_others}%)')\n",
    "\n",
    "    \n",
    "def upsampling_auto(X,X_original,Y,up_ratio):\n",
    "    check_label_distribution(Y)\n",
    "    zero_idx = np.where(Y == 0)[0]\n",
    "    one_idx = np.where(Y == 1)[0]\n",
    "    other_idx = np.where((Y != 1) & (Y != 0))[0]\n",
    "    if len(other_idx > 0):\n",
    "        return X,Y\n",
    "    repeated_data_X = np.tile(X[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_X_original = np.tile(X_original[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_Y = np.tile(Y[one_idx], (up_ratio))\n",
    "\n",
    "    X_upsampled = np.vstack((X[zero_idx], repeated_data_X))\n",
    "    X_original_upsampled = np.vstack((X_original[zero_idx], repeated_data_X_original))\n",
    "\n",
    "    Y_upsampled = np.concatenate((Y[zero_idx], repeated_data_Y)) \n",
    "    return X_upsampled, X_original_upsampled, Y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: numpy\n",
    "    feature_name_list : List\n",
    "    select_feature_list : List   (必須是feature_name_list的子集)\n",
    "Output\n",
    "    select_feature_list data\n",
    "\"\"\"\n",
    "def select_features(X, feature_name_list, select_feature_list):\n",
    "    invalid_features = set(select_feature_list) - set(feature_name_list)\n",
    "    if invalid_features:\n",
    "        raise ValueError(f\"Invalid features in select_feature_list: {invalid_features}\")\n",
    "    selected_feature_indices = [feature_name_list.index(feature) for feature in select_feature_list]\n",
    "    X_selected = X[:, :, selected_feature_indices]\n",
    "\n",
    "    return X_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958621ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    dataset_dict: Mydataset \n",
    "    loader_dict: Dataloader\n",
    "    feature_name_list: List\n",
    "    select_feature_list: List  => 若為空，則擷取所有特徵\n",
    "    batch_size: 256\n",
    "\n",
    "Output:\n",
    "    dataset_dict\n",
    "    loader_dict\n",
    "    feature_name_list ==>\n",
    "\"\"\"\n",
    "\n",
    "def read_data(task_name_list, data_date ,data_type, select_feature_list = [], batch_size = 256,use_upsample = False):\n",
    "    #檔案位置\n",
    "    #data_path = \"./data/sample/standard_data\"\n",
    "    \n",
    "    #data_path = \"C:/Users/USER/M1326168/MIMIC/DNR/20250219/data/sample/standard_data\"\n",
    "    data_path = \"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/standard_data\"\n",
    "\n",
    "    #Feature name\n",
    "    #df_feature = pd.read_csv(\"./data/sample/full_feature_name.csv\")\n",
    "    #df_feature = pd.read_csv(\"C:/Users/USER/M1326168/MIMIC/DNR/20250219/data/sample/full_feature_name.csv\")\n",
    "    df_feature = pd.read_csv(\"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/full_feature_name.csv\")   \n",
    "    feature_name_list = df_feature.columns.to_list()\n",
    "    \n",
    "    #Dataset\n",
    "    dataset_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        X_scalar = np.load(f\"{data_path}/{data_type}_scalar_X_{task_name}.npy\", allow_pickle=True)\n",
    "        X_original = np.load(f\"{data_path}/{data_type}_X_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if len(select_feature_list)>0:\n",
    "            X_scalar = select_features(X_scalar,feature_name_list,select_feature_list)\n",
    "            X_original = select_features(X_original,feature_name_list,select_feature_list)\n",
    "            assert X_scalar.shape[2] == len(select_feature_list)\n",
    "            assert X_original.shape[2] == len(select_feature_list)\n",
    "\n",
    "        Y = np.load(f\"{data_path}/{data_type}_Y_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if use_upsample:\n",
    "            if task_name == 'DNR' and data_type != 'test':\n",
    "                X_scalar,X_original,Y = upsampling_auto(X_scalar,X_original,Y,2)\n",
    "                \n",
    "        dataset_dict[task_name] = MyDataset(X_scalar,X_original,Y)\n",
    "    \n",
    "    #Dataloader\n",
    "    loader_dict = {}\n",
    "    for key, dataset in dataset_dict.items():        \n",
    "        loader_dict[key] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataset_dict,loader_dict,feature_name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70496de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#輸入多任務模型 => 拆解回多個單任務模型\n",
    "def MTL_to_STL(multi_task_model,input_dim):\n",
    "    single_task_models = {}\n",
    "    \n",
    "    for task_index, task_name in enumerate(multi_task_model.task_name_list):\n",
    "        \n",
    "        single_task_model = RNN_MTL(input_dim,[task_name]).to(device)\n",
    "        \n",
    "        #Bottom\n",
    "        # 複製bi_lstm的權重和偏差到model2的對應層中\n",
    "        single_task_model.bi_lstm.weight_ih_l0.data = multi_task_model.bi_lstm.weight_ih_l0.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l0.data = multi_task_model.bi_lstm.weight_hh_l0.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l0.data = multi_task_model.bi_lstm.bias_ih_l0.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l0.data = multi_task_model.bi_lstm.bias_hh_l0.data.clone()\n",
    "        \n",
    "        single_task_model.bi_lstm.weight_ih_l1.data = multi_task_model.bi_lstm.weight_ih_l1.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l1.data = multi_task_model.bi_lstm.weight_hh_l1.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l1.data = multi_task_model.bi_lstm.bias_ih_l1.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l1.data = multi_task_model.bi_lstm.bias_hh_l1.data.clone()\n",
    "        \n",
    "        single_task_model.bi_lstm.weight_ih_l0_reverse.data = multi_task_model.bi_lstm.weight_ih_l0_reverse.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l0_reverse.data = multi_task_model.bi_lstm.weight_hh_l0_reverse.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l0_reverse.data = multi_task_model.bi_lstm.bias_ih_l0_reverse.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l0_reverse.data = multi_task_model.bi_lstm.bias_hh_l0_reverse.data.clone()\n",
    "        \n",
    "        single_task_model.bi_lstm.weight_ih_l1_reverse.data = multi_task_model.bi_lstm.weight_ih_l1_reverse.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l1_reverse.data = multi_task_model.bi_lstm.weight_hh_l1_reverse.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l1_reverse.data = multi_task_model.bi_lstm.bias_ih_l1_reverse.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l1_reverse.data = multi_task_model.bi_lstm.bias_hh_l1_reverse.data.clone()\n",
    "        \n",
    "        #Tower\n",
    "        single_task_model.task_fc0[0].weight.data = multi_task_model.task_fc0[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc0[0].bias.data = multi_task_model.task_fc0[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_model.task_fc1[0].weight.data = multi_task_model.task_fc1[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc1[0].bias.data = multi_task_model.task_fc1[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_models[task_name] = single_task_model\n",
    "        \n",
    "    return single_task_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"            \n",
    "儲存模型參數\n",
    "\"\"\"\n",
    "def save_model(task_name_list, model_parm_dict, time=1):\n",
    "    #path = './model_parm'\n",
    "    \n",
    "    #path = 'C:/Users/USER/M1326168/MIMIC/DNR/20250219/model_parm'\n",
    "    path = 'C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm'\n",
    "    for task_name in task_name_list:\n",
    "        model_parm = model_parm_dict[task_name].state_dict().copy()\n",
    "        torch.save(model_parm, f'{path}/{task_name}_{time}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a371a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    experiment_time, \n",
    "    max_epoch, \n",
    "    learning_rate, \n",
    "    input_dim, \n",
    "    task_name_list, \n",
    "    train_loader_dict, \n",
    "    val_dataset_dict, \n",
    "    test_dataset_dict, \n",
    "    device,\n",
    "    is_show = True \n",
    "\n",
    "Output:\n",
    "    df_grade\n",
    "    stl_model_dict\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_and_test_model(experiment_time, max_epoch, learning_rate, input_dim, task_name_list, train_loader_dict, val_dataset_dict, test_dataset_dict, device,is_show = True):\n",
    "    df_grade = pd.DataFrame(columns=['time', 'task', 'auroc', 'acc', 'f1', 'pre', 'recall', 'brier_score', 'loss'])\n",
    "    best_model_params = {}\n",
    "    global_best_AUC = 0\n",
    "    global_best_loss = 10000\n",
    "    best_model_dict = {} \n",
    "    \n",
    "    count = 1\n",
    "    local_indicator = 'auroc'\n",
    "    global_indicator = 'loss'\n",
    "    \n",
    "\n",
    "    for time in range(experiment_time):\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        auc_list = []    \n",
    "        local_best_AUC = 0\n",
    "        local_best_loss = 10000\n",
    "        local_best_model_dict = {} \n",
    "        patience_counter = 0\n",
    "        \n",
    "        model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)\n",
    "        # ++\n",
    "        for epoch in tqdm(range(max_epoch)):\n",
    "            if is_show:\n",
    "                print(f'Time:{time+1}/{experiment_time} - Epoch:{epoch+1}/{max_epoch}...')\n",
    "\n",
    "            train_loss = train(model, train_loader_dict, loss_func, optimizer)\n",
    "            val_score_dict, result = test(model, val_dataset_dict, loss_func, is_show= is_show)\n",
    "\n",
    "            train_loss_list.append(train_loss)\n",
    "            val_loss_list.append(result['total_loss'])\n",
    "            auc_list.append(result['total_auc'])\n",
    "            \n",
    "            ########################################################################################################################\n",
    "            #每個任務都保留當前epoch最佳的多任務模型(local best)\n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in local_best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = copy.deepcopy(model)\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    local_best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = copy.deepcopy(model)\n",
    "                            local_best_model_dict[task_name]['id'] = count                           \n",
    "                    else:\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = copy.deepcopy(model)\n",
    "                            local_best_model_dict[task_name]['id'] = count\n",
    "            \n",
    "            #所有實驗裡最佳紀錄(best)\n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = copy.deepcopy(model)\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = copy.deepcopy(model)\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "                    else:\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = copy.deepcopy(model)\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "            count+=1\n",
    "            ########################################################################################################################\n",
    "            \"\"\" Early stop \"\"\"\n",
    "            if global_indicator == 'loss':\n",
    "                if result['total_loss'] < local_best_loss:\n",
    "                    local_best_loss = result['total_loss']\n",
    "                    if local_best_loss < global_best_loss:\n",
    "                        global_best_loss = local_best_loss\n",
    "                        best_model_params = model.state_dict().copy() \n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1     \n",
    "            else:\n",
    "                if result['total_auc'] > local_best_AUC:\n",
    "                    local_best_AUC = result['total_auc']\n",
    "                    if local_best_AUC > global_best_AUC:\n",
    "                        global_best_AUC = local_best_AUC\n",
    "                        best_model_params = model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1 \n",
    "            \n",
    "            global limit_early_stop_count\n",
    "            if patience_counter >= limit_early_stop_count:\n",
    "                break\n",
    "            ########################################################################################################################\n",
    "            \n",
    "        test_score_dict, result = test2(local_best_model_dict, test_dataset_dict, loss_func, is_show = True)\n",
    "        ############################################################################################################################\n",
    "        #每次實驗，每個任務都保留最佳model\n",
    "        save_model_dict = {}\n",
    "        for task_name in task_name_list:\n",
    "            local_stl_model_dict = MTL_to_STL(local_best_model_dict[task_name]['model'],input_dim)\n",
    "            save_model_dict[task_name] = local_stl_model_dict[task_name]        \n",
    "        save_model(task_name_list, save_model_dict, time+1)\n",
    "        ############################################################################################################################\n",
    "        print(\"最終分數\")\n",
    "        for task_name in task_name_list:\n",
    "            test_score_dict[task_name]['time'] = time + 1\n",
    "            df_grade = pd.concat([df_grade, pd.DataFrame.from_records([test_score_dict[task_name]])])\n",
    "        print(df_grade)\n",
    "        print('----------finished----------')\n",
    "    \n",
    "    global_stl_model_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        local_stl_model_dict = MTL_to_STL(best_model_dict[task_name]['model'],input_dim)    \n",
    "        global_stl_model_dict[task_name] = local_stl_model_dict[task_name]\n",
    "\n",
    "\n",
    "    #predictions = test2(local_best_model_dict, test_dataset_dict, loss_func, is_show = True)\n",
    "\n",
    "    # 取出某個 task 的預測標籤\n",
    "    # task_name = \"DNR\"\n",
    "    # print(\"真實標籤 (Y_true):\", predictions[task_name]['Y_true'].flatten()[:10])\n",
    "    # print(\"預測機率 (Y_pred_prob):\", predictions[task_name]['Y_pred_prob'].flatten()[:10])\n",
    "    # print(\"預測標籤 (Y_pred_label):\", predictions[task_name]['Y_pred_label'].flatten()[:10])\n",
    "    \n",
    "    return df_grade, global_stl_model_dict, best_model_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31150f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91391a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cbcbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "select_feature_list = []\n",
    "top_percent = 0.9\n",
    "remove_time_count = 0\n",
    "full_result_dict = {}\n",
    "featurn_count_list = []\n",
    "\n",
    "########################################################################################################################\n",
    "remove_time_count += 1\n",
    "\"\"\"\n",
    "read data\n",
    "\"\"\"\n",
    "train_dataset_dict,train_loader_dict,feature_name_list = read_data(task_name_list,'','train',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "val_dataset_dict,val_loader_dict,_ = read_data(task_name_list,'','validation',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "test_dataset_dict,test_loader_dict,_ = read_data(task_name_list,'','test',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "if len(select_feature_list)!=0:\n",
    "    feature_name_list = select_feature_list\n",
    "input_dim = train_dataset_dict[task_name_list[0]].inputs.numpy().shape[2]\n",
    "print(f'==> input_dim: {input_dim}')\n",
    "featurn_count_list.append(input_dim)\n",
    "########################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "print(\"training\")\n",
    "df_grade, stl_model_dict, best_model_dict = train_and_test_model(experiment_time = experiment_time, \n",
    "                                                 max_epoch = max_epoch, \n",
    "                                                 learning_rate = learning_rate, \n",
    "                                                 input_dim = input_dim, \n",
    "                                                 task_name_list = task_name_list, \n",
    "                                                 train_loader_dict = train_loader_dict, \n",
    "                                                 val_dataset_dict = val_dataset_dict, \n",
    "                                                 test_dataset_dict = test_dataset_dict, \n",
    "                                                 device = device, is_show = False)\n",
    "df_grade['remove_time'] = remove_time_count\n",
    "full_result_dict[remove_time_count] = {}\n",
    "full_result_dict[remove_time_count]['result'] = df_grade\n",
    "full_result_dict[remove_time_count]['model'] = stl_model_dict\n",
    "full_result_dict[remove_time_count]['select_feature_list'] = feature_name_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b30229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration_curve\n",
    "#彩色版 ，棄用\n",
    "#需手動操作\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "#跑第二次要註解掉\n",
    "# def save_predictions(predictions, filename):\n",
    "#      np.save(filename, predictions)\n",
    "\n",
    "\n",
    "def calibration(y_true, y_prob):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Calibration curve')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect calibration')\n",
    "    plt.xlabel('Average predicted probability')\n",
    "    plt.ylabel('Ratio of positives')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend()\n",
    "    plt.show()   \n",
    "\n",
    "def run_calibration(local_best_model_dict, dataset_dict):\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    calibration_results = {}\n",
    "\n",
    "    for task_name in task_name_list:\n",
    "        model = local_best_model_dict[task_name]['model']\n",
    "        model.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        \n",
    "        prob = model.predict_prob(X)[task_name].cpu().detach().numpy()\n",
    "        preds = Y.flatten()\n",
    "        \n",
    "        calibration(preds, prob)  # 傳遞兩個參數\n",
    "\n",
    "        \n",
    "        # 儲存預測結果，跑第二次記得註解掉\n",
    "        #save_predictions(prob, f\"C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/{task_name}_predictions.npy\")\n",
    "\n",
    "    return calibration_results\n",
    "\n",
    "print(\"Running Calibration on Test Data...\")\n",
    "calibration_results = run_calibration(best_model_dict, test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a64910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob, model_name, n_bins=10, save_path=None):\n",
    "    \"\"\"\n",
    "    繪製校準曲線 (Calibration Curve)\n",
    "    :param y_true: 真實標籤\n",
    "    :param y_prob: 預測機率\n",
    "    :param model_name: 模型名稱\n",
    "    :param n_bins: 分箱數量\n",
    "    :param save_path: 如果提供路徑，將圖表儲存為檔案\n",
    "    \"\"\"\n",
    "    # 計算校準曲線\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins)\n",
    "\n",
    "    # 繪製校準曲線\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', color='blue', lw = 5)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='black', lw = 4)\n",
    "\n",
    "    # 設定字體大小和樣式\n",
    "    font_properties = {'size': 24, 'family': 'Arial', 'weight': 'bold'}\n",
    "    plt.xlabel('Average Predicted Probability', fontdict=font_properties)\n",
    "    plt.ylabel('Observed Proportion', fontdict=font_properties)\n",
    "    #plt.title(f'Calibration Curve - {model_name}', fontdict=font_properties)\n",
    "    plt.legend(loc=\"best\", prop={'size': 24, 'family': 'Arial', 'weight': 'bold'})\n",
    "\n",
    "    # 設定刻度字體大小\n",
    "    plt.xticks(fontsize=18, fontfamily='Arial', fontweight='bold')\n",
    "    plt.yticks(fontsize=18, fontfamily='Arial', fontweight='bold')\n",
    "\n",
    "    # 儲存或顯示圖表\n",
    "    # if save_path:\n",
    "    #     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    #     print(f\"Calibration curve saved to {save_path}\")\n",
    "    # else:\n",
    "    #     plt.show()\n",
    "    output_path = f'C:/Users/USER/M1326168/MIMIC/DNR/20250528/calibration_curve.tiff'\n",
    "    plt.savefig(output_path, format='tiff', dpi=300, bbox_inches='tight')\n",
    "    print(f\"圖表已儲存為 TIFF 檔案：{output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def run_calibration_on_tasks(local_best_model_dict, dataset_dict):\n",
    "    \"\"\"\n",
    "    為每個任務繪製校準曲線\n",
    "    :param local_best_model_dict: 包含最佳模型的字典\n",
    "    :param dataset_dict: 測試數據集字典\n",
    "    \"\"\"\n",
    "    for task_name in dataset_dict.keys():\n",
    "        # model = local_best_model_dict[task_name]['model']\n",
    "        model = local_best_model_dict['DNR']['model']\n",
    "        model.eval()\n",
    "        # X = dataset_dict[task_name].inputs.numpy()\n",
    "        # Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        X = dataset_dict['DNR'].inputs.numpy()\n",
    "        Y = dataset_dict['DNR'].labels.unsqueeze(1).numpy()\n",
    "\n",
    "        # 獲取預測機率\n",
    "        # prob = model.predict_prob(X)[task_name].cpu().detach().numpy()\n",
    "        prob = model.predict_prob(X)['DNR'].cpu().detach().numpy()\n",
    "\n",
    "        # 繪製校準曲線\n",
    "        plot_calibration_curve(Y.flatten(), prob.flatten(), model_name=task_name)\n",
    "\n",
    "# 呼叫函數進行測試\n",
    "print(\"Running Calibration Curve on Test Data...\")\n",
    "run_calibration_on_tasks(best_model_dict, test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(labels, preds, model_name, save_path=None):\n",
    "    \"\"\"\n",
    "    繪製 ROC 曲線\n",
    "    :param labels: 真實標籤\n",
    "    :param preds: 預測機率\n",
    "    :param model_name: 模型名稱\n",
    "    :param save_path: 如果提供路徑，將圖表儲存為檔案\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # plt.plot(fpr, tpr, color='black', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    # plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1, label='Chance')\n",
    "    #plt.plot(fpr, tpr, color='black', lw=2)\n",
    "    plt.plot(fpr, tpr, color='blue', lw=5)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--', lw=4)\n",
    "    \n",
    "    # 設定字體大小和樣式\n",
    "    font_properties = {'size': 24, 'family': 'Arial', 'weight': 'bold'}\n",
    "    plt.xlabel('False Positive Rate', fontdict=font_properties)\n",
    "    plt.ylabel('True Positive Rate', fontdict=font_properties)\n",
    "    #plt.title(f'ROC Curve - {model_name}', fontdict=font_properties)\n",
    "    plt.legend(loc=\"lower right\", prop={'size': 24, 'family': 'Arial', 'weight': 'bold'})\n",
    "    \n",
    "    # 設定刻度字體大小\n",
    "    plt.xticks(fontsize=18, fontfamily='Arial', fontweight='bold')\n",
    "    plt.yticks(fontsize=18, fontfamily='Arial', fontweight='bold')\n",
    "    \n",
    "    # # 儲存或顯示圖表\n",
    "    # if save_path:\n",
    "    #     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    #     print(f\"ROC curve saved to {save_path}\")\n",
    "    # else:\n",
    "    #     plt.show()\n",
    "\n",
    "    output_path = f'C:/Users/USER/M1326168/MIMIC/DNR/20250528/ROC_curve.tiff'\n",
    "    plt.savefig(output_path, format='tiff', dpi=300, bbox_inches='tight')\n",
    "    print(f\"圖表已儲存為 TIFF 檔案：{output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def run_roc_curve_on_tasks(local_best_model_dict, dataset_dict):\n",
    "    \"\"\"\n",
    "    為每個任務繪製 ROC 曲線\n",
    "    \"\"\"\n",
    "    for task_name in dataset_dict.keys():\n",
    "        #model = local_best_model_dict[task_name]['model']\n",
    "        model = local_best_model_dict['DNR']['model']\n",
    "        model.eval()\n",
    "        # X = dataset_dict[task_name].inputs.numpy()\n",
    "        # Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        X = dataset_dict['DNR'].inputs.numpy()\n",
    "        Y = dataset_dict['DNR'].labels.unsqueeze(1).numpy()\n",
    "        \n",
    "        # 獲取預測機率\n",
    "        # prob = model.predict_prob(X)[task_name].cpu().detach().numpy()\n",
    "        prob = model.predict_prob(X)['DNR'].cpu().detach().numpy()\n",
    "        \n",
    "        # 繪製 ROC 曲線\n",
    "        plot_roc_curve(Y.flatten(), prob.flatten(), model_name=task_name)\n",
    "\n",
    "# 呼叫函數進行測試\n",
    "print(\"Running ROC curve on Test Data...\")\n",
    "run_roc_curve_on_tasks(best_model_dict, test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35685abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "#彩色版 ，棄用\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_roc_curve(labels, preds, model_name, save_path=None):\n",
    "#     \"\"\"\n",
    "#     繪製 ROC 曲線\n",
    "#     :param labels: 真實標籤\n",
    "#     :param preds: 預測機率\n",
    "#     :param model_name: 模型名稱\n",
    "#     :param save_path: 如果提供路徑，將圖表儲存為檔案\n",
    "#     \"\"\"\n",
    "#     fpr, tpr, _ = roc_curve(labels, preds)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     # plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "#     # plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=2)\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title(f'ROC Curve - {model_name}')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "    \n",
    "#     if save_path:\n",
    "#         plt.savefig(save_path)\n",
    "#         print(f\"ROC curve saved to {save_path}\")\n",
    "#     else:\n",
    "#         plt.show()\n",
    "\n",
    "# def run_roc_curve_on_tasks(local_best_model_dict, dataset_dict):\n",
    "#     \"\"\"\n",
    "#     為每個任務繪製 ROC 曲線\n",
    "#     \"\"\"\n",
    "#     for task_name in dataset_dict.keys():\n",
    "#         model = local_best_model_dict[task_name]['model']\n",
    "#         model.eval()\n",
    "#         X = dataset_dict[task_name].inputs.numpy()\n",
    "#         Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        \n",
    "#         # 獲取預測機率\n",
    "#         prob = model.predict_prob(X)[task_name].cpu().detach().numpy()\n",
    "        \n",
    "#         # 繪製 ROC 曲線\n",
    "#         plot_roc_curve(Y.flatten(), prob.flatten(), model_name=task_name)\n",
    "\n",
    "# print(\"Running ROC curve on Test Data...\")\n",
    "# run_roc_curve_on_tasks(best_model_dict, test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delong test\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "def load_predictions(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "def delong_test(preds1, preds2, labels):\n",
    "    def _auc(X, Y):\n",
    "        return 1 / (len(X) * len(Y)) * sum([_kernel(x, y) for x in X for y in Y])\n",
    "\n",
    "    def _kernel(x, y):\n",
    "        return 0.5 if x == y else int(y < x)\n",
    "\n",
    "    def _structural_components(X, Y):\n",
    "        V10 = [1 / len(Y) * sum([_kernel(x, y) for y in Y]) for x in X]\n",
    "        V01 = [1 / len(X) * sum([_kernel(x, y) for x in X]) for y in Y]\n",
    "        return V10, V01\n",
    "\n",
    "    def _get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "        return 1 / (len(V_A) - 1) * sum([(a - auc_A) * (b - auc_B) for a, b in zip(V_A, V_B)])\n",
    "    \n",
    "    X_A = [p for p, a in zip(preds1, labels) if a == 1]\n",
    "    Y_A = [p for p, a in zip(preds1, labels) if a == 0]\n",
    "    X_B = [p for p, a in zip(preds2, labels) if a == 1]\n",
    "    Y_B = [p for p, a in zip(preds2, labels) if a == 0]\n",
    "    \n",
    "    auc_A = _auc(X_A, Y_A)\n",
    "    auc_B = _auc(X_B, Y_B)\n",
    "\n",
    "    V_A10, V_A01 = _structural_components(X_A, Y_A)\n",
    "    V_B10, V_B01 = _structural_components(X_B, Y_B)\n",
    "\n",
    "    var_A = (_get_S_entry(V_A10, V_A10, auc_A, auc_A) / len(V_A10) +\n",
    "             _get_S_entry(V_A01, V_A01, auc_A, auc_A) / len(V_A01))\n",
    "    var_B = (_get_S_entry(V_B10, V_B10, auc_B, auc_B) / len(V_B10) +\n",
    "             _get_S_entry(V_B01, V_B01, auc_B, auc_B) / len(V_B01))\n",
    "    covar_AB = (_get_S_entry(V_A10, V_B10, auc_A, auc_B) / len(V_A10) +\n",
    "                _get_S_entry(V_A01, V_B01, auc_A, auc_B) / len(V_A01))\n",
    "    \n",
    "    z = (auc_A - auc_B) / ((var_A + var_B - 2 * covar_AB)**0.5 + 1e-8)\n",
    "    p = st.norm.sf(abs(z)) * 2\n",
    "    \n",
    "    return auc_A, auc_B, z, p\n",
    "\n",
    "def run_delong_test_on_tasks(local_best_model_dict, dataset_dict, task_name_list1, task_name_list2):\n",
    "    preds1 = load_predictions('C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/DNR_predictions.npy')\n",
    "    preds2 = []\n",
    "    labels = None\n",
    "    \n",
    "    for task_name in task_name_list2:\n",
    "        model = local_best_model_dict[task_name]['model']\n",
    "        model.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        \n",
    "        prob = model.predict_prob(X)[task_name].cpu().detach().numpy()\n",
    "        preds2.extend(prob.flatten())\n",
    "        if labels is None:\n",
    "            labels = Y.flatten()\n",
    "    \n",
    "    auc_A, auc_B, z, p = delong_test(preds1, preds2, labels)\n",
    "    \n",
    "    delong_results = {'auc_model1': auc_A, 'auc_model2': auc_B, 'z_score': z, 'p_value': p}\n",
    "    print(f\"AUC Model of single-task learning: {auc_A:.5f}\")\n",
    "    #print(f\"AUC Model of 3 task: {auc_A:.5f}\")\n",
    "    #print(f\"AUC Model of multi-task learning: {auc_B:.5f}\")\n",
    "    print(f\"AUC Model of 3 task learning: {auc_B:.5f}\")\n",
    "    #print(f\"AUC Model of 5 task learning: {auc_B:.5f}\")\n",
    "    print(f\"z-score: {z:.5f}\")\n",
    "    print(f\"p-value: {p:.5f}\")\n",
    "    \n",
    "    return delong_results\n",
    "\n",
    "print(\"Running DeLong Test on Test Data...\")\n",
    "task_name_list1 = ['DNR']\n",
    "#task_name_list1 = ['DNR','dod_30day','Vasopressor']\n",
    "#task_name_list2 = ['DNR']\n",
    "#task_name_list2 = ['DNR','dod_30day','Vasopressor','InvasiveVent','dialysis']\n",
    "task_name_list2 = ['DNR','dod_30day','Vasopressor']\n",
    "delong_results = run_delong_test_on_tasks(best_model_dict, test_dataset_dict, task_name_list1, task_name_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343530f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def calculate_net_benefit_model(thresh_group, y_pred_score, y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        y_pred_label = y_pred_score > thresh\n",
    "        tn, fp, fn, tp = confusion_matrix(y_label, y_pred_label).ravel()\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n",
    "        net_benefit_model = np.append(net_benefit_model, net_benefit)\n",
    "    return net_benefit_model\n",
    "\n",
    "\n",
    "def calculate_net_benefit_all(thresh_group, y_label):\n",
    "    net_benefit_all = np.array([])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_label, y_label).ravel()\n",
    "    total = tp + tn\n",
    "    for thresh in thresh_group:\n",
    "        net_benefit = (tp / total) - (tn / total) * (thresh / (1 - thresh))\n",
    "        net_benefit_all = np.append(net_benefit_all, net_benefit)\n",
    "    return net_benefit_all\n",
    "def plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all , model_id,model_name_list):\n",
    "    \n",
    "    choose_colar = 'darkorange'\n",
    "    if model_id == 1:\n",
    "        choose_colar = 'blue'\n",
    "    elif model_id == 2:\n",
    "        choose_colar = 'green'\n",
    "    else:\n",
    "        choose_colar = 'red'\n",
    "    \n",
    "    choose_colar = 'black'\n",
    "    #Plot\n",
    "\n",
    "    #ax.plot(thresh_group, net_benefit_model, color = choose_colar)\n",
    "    ax.plot(thresh_group, net_benefit_model, color = 'blue',lw = 5)\n",
    "\n",
    "    #ax.plot(thresh_group, net_benefit_model, color = choose_colar, label = f'{model_name_list[model_id]}')\n",
    "    \n",
    "    if model_id == 1:\n",
    "        #ax.plot(thresh_group, net_benefit_all, color = 'black',label = 'Treat all')\n",
    "        #ax.plot((0, 1), (0, 0), color = 'black', linestyle = ':', label = 'Treat none')\n",
    "        ax.plot(thresh_group, net_benefit_all, color = 'black',lw = 5)\n",
    "        ax.plot((0, 1), (0, 0), color = 'black', linestyle = ':',lw = 5)\n",
    "    ax.plot(thresh_group, net_benefit_all, color = 'black',lw = 4)\n",
    "    ax.plot((0, 1), (0, 0), color = 'black', linestyle = ':',lw = 4)\n",
    "    \n",
    "    #Fill，显示出模型较于treat all和treat none好的部分\n",
    "    y2 = np.maximum(net_benefit_all, 0)\n",
    "    y1 = np.maximum(net_benefit_model, y2)\n",
    "    ax.fill_between(thresh_group, y1, y2, color = 'black', alpha = 0.2)\n",
    "    \n",
    "    font_properties = {'size': 24,  'family': 'Arial', 'fontweight':'bold'}\n",
    "    plt.xticks(fontproperties='Arial', **font_properties)\n",
    "    plt.yticks(fontproperties='Arial', **font_properties)\n",
    "    \n",
    "    #Figure Configuration， 美化一下细节\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(net_benefit_model.min() - 0.15, net_benefit_model.max() + 0.15)#adjustify the y axis limitation\n",
    "    ax.set_xlabel(\n",
    "        xlabel = 'Threshold Probability', \n",
    "        fontdict= {'fontfamily':'Arial', 'fontsize': 24, 'fontweight':'bold'}\n",
    "        )\n",
    "    ax.set_ylabel(\n",
    "        ylabel = 'Net Benefit', \n",
    "        fontdict= {'fontfamily':'Arial', 'fontsize': 24, 'fontweight':'bold'}\n",
    "        )\n",
    "    \n",
    "    #ax.grid('major')\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "    #ax.legend(loc = 'upper right')\n",
    "    plt.legend().set_visible(False)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision_curve\n",
    "def decision_curve(model_list, model_name_list, x, y, task_name):\n",
    "    \"\"\"\n",
    "    繪製決策曲線\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        \n",
    "    assert len(model_list) == len(model_name_list)\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(model_list)):\n",
    "        model = model_list[i]\n",
    "        model.eval()\n",
    "        if torch.is_tensor(x):\n",
    "            x = x.cpu().numpy()\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "        y_label = y\n",
    "\n",
    "        # 模型預測\n",
    "        out = model(x)\n",
    "        if isinstance(out, dict):  # 如果輸出是字典\n",
    "            print(f\"Model output keys: {out.keys()}\")  # 打印字典的鍵\n",
    "            # 修改這裡，根據實際鍵名稱提取值\n",
    "            y_pred_score = out.get('DNR', None)  # 使用 .get() 避免 KeyError\n",
    "            if y_pred_score is None:\n",
    "                raise KeyError(\"The key 'logits' was not found in the model output. Please check the output keys.\")\n",
    "        else:  # 如果輸出是張量\n",
    "            y_pred_score = out\n",
    "\n",
    "        y_pred_score = y_pred_score.float().cpu().detach().numpy()\n",
    "        y_pred = (y_pred_score > 0.5).astype(int)\n",
    "        \n",
    "        # 計算 Net Benefit\n",
    "        thresh_group = np.arange(0, 1, 0.01)\n",
    "        net_benefit_model = calculate_net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "        net_benefit_all = calculate_net_benefit_all(thresh_group, y_label)\n",
    "\n",
    "        # 繪製決策曲線\n",
    "        plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all, i, model_name_list)\n",
    "\n",
    "    # plt.show()\n",
    "    output_path = f'C:/Users/USER/M1326168/MIMIC/DNR/20250528/decision_curve.tiff'\n",
    "    plt.savefig(output_path, format='tiff', dpi=300, bbox_inches='tight')\n",
    "    print(f\"圖表已儲存為 TIFF 檔案：{output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# 呼叫 decision_curve\n",
    "for task_name in task_name_list2:\n",
    "    model = best_model_dict[task_name]['model']\n",
    "    model.eval()\n",
    "    X = test_dataset_dict[task_name].inputs.numpy()\n",
    "    Y = test_dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "    \n",
    "    # 呼叫 decision_curve，傳入正確的參數\n",
    "    decision_curve(\n",
    "        model_list=[model],  # 單一模型列表\n",
    "        #model_name_list=[task_name],  # 單一模型名稱列表\n",
    "        model_name_list=['DNR'],  # 單一模型名稱列表\n",
    "        x=X,  # 測試資料的輸入\n",
    "        y=Y,  # 測試資料的標籤\n",
    "        task_name=task_name  # 任務名稱\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#棄用\n",
    "# #decision_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# def decision_curve_analysis(y_true, y_prob, thresholds=np.arange(0.0, 1.01, 0.01)):\n",
    "#     net_benefit = []\n",
    "#     for threshold in thresholds:\n",
    "#         tp = np.sum((y_prob >= threshold) & (y_true == 1))\n",
    "#         fp = np.sum((y_prob >= threshold) & (y_true == 0))\n",
    "#         fn = np.sum((y_prob < threshold) & (y_true == 1))\n",
    "#         tn = np.sum((y_prob < threshold) & (y_true == 0))       \n",
    "#         n = len(y_true)\n",
    "#         p = np.sum(y_true)\n",
    "#         nb = (tp / n) - (fp / n) * (threshold / (1 - threshold))\n",
    "#         net_benefit.append(nb)\n",
    "    \n",
    "#     return thresholds, net_benefit\n",
    "\n",
    "# def plot_decision_curve(y_true, y_prob, model_name):\n",
    "#     thresholds, net_benefit = decision_curve_analysis(y_true, y_prob)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(thresholds, net_benefit, label=f'{model_name} Decision Curve')\n",
    "#     plt.plot(thresholds, thresholds * (np.sum(y_true) / len(y_true)), linestyle='--', label='Treat All')\n",
    "#     plt.plot(thresholds, np.zeros_like(thresholds), linestyle='--', label='Treat None')\n",
    "#     plt.xlabel('Threshold Probability')\n",
    "#     plt.ylabel('Net Benefit')\n",
    "#     plt.title('Decision Curve Analysis')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# # 繪製決策曲線\n",
    "# for task_name in task_name_list2:\n",
    "#     model = best_model_dict[task_name]['model']\n",
    "#     model.eval()\n",
    "#     X = test_dataset_dict[task_name].inputs.numpy()\n",
    "#     Y = test_dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "#     prob = model.predict_prob(X)[task_name].cpu().detach().numpy()\n",
    "#     plot_decision_curve(Y.flatten(), prob.flatten(), task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "\n",
    "def group_result(df):\n",
    "    agg_columns = {\n",
    "        'acc': ['mean', 'std'],\n",
    "        'pre': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'auroc': ['mean', 'std'],\n",
    "        'brier_score': ['mean', 'std']\n",
    "    }\n",
    "    df_group = df.groupby('task').agg(agg_columns)\n",
    "    df_group.columns = [f\"{col[0]}_{col[1]}\" for col in df_group.columns]\n",
    "    for metric in ['acc', 'pre', 'f1', 'recall', 'auroc','brier_score']:\n",
    "        df_group[f\"{metric}_combined\"] = df_group.apply(\n",
    "            lambda row: f\"{row[f'{metric}_mean']:.4f} ± {row[f'{metric}_std']:.4f}\", axis=1\n",
    "        )\n",
    "    df_result = df_group[[f\"{metric}_combined\" for metric in ['acc', 'pre', 'f1', 'recall', 'auroc','brier_score']]]\n",
    "    df_result.reset_index(inplace=True)\n",
    "    df_result.columns = ['task','acc', 'pre', 'f1', 'recall', 'auroc','brier_score']\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def save_to_xlsx(df_save,file_name = 'output'):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(df_save, index=False, header=True), 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "    #wb.save(f'./model_parm/{file_name}.xlsx')model.\n",
    "    wb.save(f'C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/{file_name}.xlsx')\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "df_grade_group = group_result(df_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d18871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b205ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48577937",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_xlsx(df_grade_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b392b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_columns = pd.read_csv(\"./data/sample/full_feature_name.csv\")\n",
    "\n",
    "#df_columns = pd.read_csv(\"C:/Users/USER/M1326168/MIMIC/DNR/20250219/data/sample/full_feature_name.csv\")\n",
    "df_columns = pd.read_csv(\"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/full_feature_name.csv\")\n",
    "temperal = 0\n",
    "feature_list = df_columns.columns.tolist()\n",
    "for i in range(len(feature_list)):\n",
    "    print(feature_list[i])\n",
    "    if df_columns.columns[i] == 'gender':\n",
    "        temperal = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'時序特徵:{temperal}  靜態特徵:{len(df_columns.columns)-temperal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4969bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

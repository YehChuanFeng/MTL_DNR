{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 1\n",
    "seq_len = 1\n",
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "max_epoch = 20\n",
    "experiment_time = 1\n",
    "limit_early_stop_count = 5\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "use_upsample = True\n",
    "use_mini_sample = True\n",
    "\n",
    "#task_name_list = ['dialysis','DNR']\n",
    "#task_name_list = ['dod_7day','DNR']\n",
    "#task_name_list = ['dod_30day','DNR']\n",
    "#task_name_list = ['dod_90day','DNR']\n",
    "#task_name_list = ['Weaning_successful','DNR']\n",
    "#task_name_list = ['SBT','DNR']\n",
    "#task_name_list = ['dod_30day']\n",
    "#task_name_list = ['dod','dod_90day']\n",
    "#task_name_list = ['dod_30day','dod_90day']\n",
    "#task_name_list = ['dod']\n",
    "#task_name_list = ['DNR']\n",
    "#task_name_list = ['Vasopressor']\n",
    "#task_name_list = ['InvasiveVent']\n",
    "#task_name_list = ['dod_90day']\n",
    "\n",
    "task_name_list = ['DNR','dod_30day','Vasopressor']\n",
    "\n",
    "#task_name_list = ['DNR','dod_30day','Vasopressor','InvasiveVent','dialysis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "##++\n",
    "class RNN_MTL(nn.Module):\n",
    "    def __init__(self, input_dim, task_name_list,window_size = 3, dropout_ratio=0.0):\n",
    "        super(RNN_MTL, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.relu = nn.ReLU()  # Activation function for hidden layers\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.task_name_list = task_name_list\n",
    "        self.num_tasks = len(task_name_list)\n",
    "        hidden_dim = [256, 64]\n",
    "        output_size = 1\n",
    "\n",
    "        # Bottom\n",
    "        self.bi_lstm = torch.nn.LSTM(input_dim, hidden_dim[0], num_layers=2, batch_first = True, bidirectional = True)\n",
    "        #self.bt_fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        #self.bt_fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        #self.bt_fc3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
    "\n",
    "        # Towers\n",
    "        self.task_fc0 = nn.ModuleList([nn.Linear(hidden_dim[0]*window_size*2, hidden_dim[1]) for _ in range(self.num_tasks)])\n",
    "        self.task_fc1 = nn.ModuleList([nn.Linear(hidden_dim[1], output_size) for _ in range(self.num_tasks)])\n",
    "    \n",
    "    def data_check(self,x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        #if x.ndim == 3:\n",
    "        #    x = x.reshape(x.shape[0], x.shape[1] * x.shape[2])  # Flatten \n",
    "        \n",
    "        x = x.to(device)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.train() \n",
    "        x = self.data_check(x)\n",
    "        \n",
    "        h, _   = self.bi_lstm(x)\n",
    "        h = torch.nn.functional.relu(h)\n",
    "        h = torch.flatten(h, start_dim = 1)\n",
    "        # Towers\n",
    "        task_out = {}\n",
    "        for task_index in range(self.num_tasks):\n",
    "            task_name = self.task_name_list[task_index]\n",
    "            hi = self.task_fc0[task_index](h)\n",
    "            hi = self.relu(hi)\n",
    "            hi = self.dropout(hi)\n",
    "            hi = self.task_fc1[task_index](hi)\n",
    "            hi = self.sigmoid(hi)\n",
    "            task_out[task_name] = hi    \n",
    "            \n",
    "        if len(self.task_name_list) == 1:\n",
    "            return task_out[self.task_name_list[0]]\n",
    "        else:\n",
    "            return task_out\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        return prob_dict\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        prob_dict = self.forward(x)\n",
    "        \n",
    "        if len(self.task_name_list) == 1:\n",
    "            prob_dict_true = {}\n",
    "            prob_dict_true[self.task_name_list[0]] = prob_dict\n",
    "            return prob_dict_true\n",
    "        \n",
    "        return prob_dict\n",
    "    \n",
    "    def predict(self, x, threshold = 0.5):\n",
    "        self.eval()\n",
    "        prob_dict = self.predict_prob(x)\n",
    "        pred_dict = {}\n",
    "        \n",
    "        for key, value in prob_dict.items():\n",
    "            #tensor轉numpy\n",
    "            value = value.cpu().detach().numpy()\n",
    "            pred_class = [1 if x > threshold else 0 for x in value]\n",
    "            pred_dict[key] = np.array(pred_class) \n",
    "        return pred_dict\n",
    "    \n",
    "    def evaluate(self,X,label,task_name,criterion):\n",
    "        with torch.no_grad():\n",
    "            prob = self.predict_prob(X)[task_name].cpu().detach().numpy() #tensor=>numpy\n",
    "            pred = self.predict(X)[task_name] \n",
    "            score = compute_scores(label,pred,prob)\n",
    "            score['task'] = task_name\n",
    "            loss = criterion(torch.from_numpy(prob).to(device),torch.from_numpy(label).to(device)).item()\n",
    "            score['loss'] = loss/len(label)\n",
    "            return score\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8537c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y_true, y_pred,y_prob):\n",
    "    if np.any(np.isnan(y_prob)):\n",
    "        print(y_prob)\n",
    "        input()\n",
    "        \n",
    "    scores = {}\n",
    "    try:\n",
    "        scores['task'] = 'Null'\n",
    "        scores['auroc'] = round(roc_auc_score(y_true, y_prob), 3)\n",
    "        scores['acc'] = round(accuracy_score(y_true, y_pred), 3)\n",
    "        scores['f1'] = round(f1_score(y_true, y_pred), 3)\n",
    "        scores['pre'] = round(precision_score(y_true, y_pred), 3)\n",
    "        scores['recall'] = round(recall_score(y_true, y_pred), 3)\n",
    "        scores['brier_score'] = round(brier_score_loss(y_true, y_prob), 3)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    model\n",
    "    dict: Mydataset\n",
    "    loss_function\n",
    "Output:\n",
    "    score: dict + dict\n",
    "    result: dict => ['total_auc','total_loss']\n",
    "\"\"\"\n",
    "def test(model, dataset_dict, criterion, is_show = True):\n",
    "    model.eval()\n",
    "\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    for task_name in task_name_list:  # 循環每個任務\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "    \n",
    "        score[task_name] = model.evaluate(X,Y,task_name,criterion)\n",
    "        \n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "            \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "    return score,result\n",
    "\n",
    "\"\"\"\n",
    "local_best_model_dict: #dict{'task_name':{'model','performance(target_score)','id'}}\n",
    "model\n",
    "\"\"\"\n",
    "def test2(local_best_model_dict, dataset_dict, criterion, is_show = True):\n",
    "    score = {}\n",
    "    result = {'total_auc': 0, 'total_loss': 0}\n",
    "    task_name_list = list(dataset_dict.keys())\n",
    "    \n",
    "    for task_name in task_name_list:\n",
    "        #modelr.load_state_dict(local_best_model_dict[task_name]['model'])\n",
    "        modelr = local_best_model_dict[task_name]['model']\n",
    "        modelr.eval()\n",
    "        X = dataset_dict[task_name].inputs.numpy()\n",
    "        Y = dataset_dict[task_name].labels.unsqueeze(1).numpy()\n",
    "        score[task_name] = modelr.evaluate(X,Y,task_name,criterion)\n",
    "        result['total_auc'] = result['total_auc'] + score[task_name]['auroc']\n",
    "        result['total_loss'] = result['total_loss'] + score[task_name]['loss']\n",
    "        \n",
    "        if is_show:\n",
    "            print(score[task_name])\n",
    "    print(X,Y)\n",
    "    return score,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d074fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, np_X_scalar,np_X_original, np_Y):\n",
    "        self.inputs = torch.from_numpy(np_X_scalar).float()\n",
    "        self.inputs_original = torch.from_numpy(np_X_original).float()\n",
    "        self.labels = torch.from_numpy(np_Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, _input, target):\n",
    "        pt = _input\n",
    "        alpha = self.alpha\n",
    "        loss = - alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1 - alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "def check_label_distribution (data_Y):\n",
    "    count_1 = np.count_nonzero(data_Y == 1)\n",
    "    count_0 = np.count_nonzero(data_Y == 0)\n",
    "    count_others = np.count_nonzero((data_Y != 1) & (data_Y != 0))\n",
    "    ratio_1 = round(count_1/len(data_Y)*100,2)\n",
    "    ratio_0 = round(count_0/len(data_Y)*100,2)\n",
    "    ratio_others = round(count_others/len(data_Y)*100,2)\n",
    "    print(f'Distribution: 1=>{count_1}({ratio_1}%),  0=>{count_0}({ratio_0}%),  others=>{count_others}({ratio_others}%)')\n",
    "\n",
    "    \n",
    "def upsampling_auto(X,X_original,Y,up_ratio):\n",
    "    check_label_distribution(Y)\n",
    "    zero_idx = np.where(Y == 0)[0]\n",
    "    one_idx = np.where(Y == 1)[0]\n",
    "    other_idx = np.where((Y != 1) & (Y != 0))[0]\n",
    "    if len(other_idx > 0):\n",
    "        return X,Y\n",
    "    repeated_data_X = np.tile(X[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_X_original = np.tile(X_original[one_idx], (up_ratio, 1, 1))\n",
    "    repeated_data_Y = np.tile(Y[one_idx], (up_ratio))\n",
    "\n",
    "    X_upsampled = np.vstack((X[zero_idx], repeated_data_X))\n",
    "    X_original_upsampled = np.vstack((X_original[zero_idx], repeated_data_X_original))\n",
    "\n",
    "    Y_upsampled = np.concatenate((Y[zero_idx], repeated_data_Y)) \n",
    "    return X_upsampled,X_original_upsampled, Y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dde32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: numpy\n",
    "    feature_name_list : List\n",
    "    select_feature_list : List   (必須是feature_name_list的子集)\n",
    "Output\n",
    "    select_feature_list data\n",
    "\"\"\"\n",
    "def select_features(X, feature_name_list, select_feature_list):\n",
    "    invalid_features = set(select_feature_list) - set(feature_name_list)\n",
    "    if invalid_features:\n",
    "        raise ValueError(f\"Invalid features in select_feature_list: {invalid_features}\")\n",
    "    selected_feature_indices = [feature_name_list.index(feature) for feature in select_feature_list]\n",
    "    X_selected = X[:, :, selected_feature_indices]\n",
    "\n",
    "    return X_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958621ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    dataset_dict: Mydataset \n",
    "    loader_dict: Dataloader\n",
    "    feature_name_list: List\n",
    "    select_feature_list: List \n",
    "    batch_size: 256\n",
    "\n",
    "Output:\n",
    "    dataset_dict\n",
    "    loader_dict\n",
    "    feature_name_list ==>\n",
    "\"\"\"\n",
    "\n",
    "def read_data(task_name_list, data_date ,data_type, select_feature_list = [], batch_size = 256,use_upsample = False):\n",
    "    #data_path = \"./data/sample/standard_data\"\n",
    "    \n",
    "    data_path = \"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/standard_data\"\n",
    "    #data_path = \"C:/Users/USER/M1326168/MIMIC/DNR/20250219/data/sample/standard_data\"\n",
    "    \n",
    "    #Feature name\n",
    "    #df_feature = pd.read_csv(\"./data/sample/full_feature_name.csv\")\n",
    "    df_feature = pd.read_csv(\"C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/full_feature_name.csv\")\n",
    "    feature_name_list = df_feature.columns.to_list()\n",
    "    \n",
    "    #Dataset\n",
    "    dataset_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        X_scalar = np.load(f\"{data_path}/{data_type}_scalar_X_{task_name}.npy\", allow_pickle=True)\n",
    "        X_original = np.load(f\"{data_path}/{data_type}_X_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if len(select_feature_list)>0:\n",
    "            X_scalar = select_features(X_scalar,feature_name_list,select_feature_list)\n",
    "            X_original = select_features(X_original,feature_name_list,select_feature_list)\n",
    "            assert X_scalar.shape[2] == len(select_feature_list)\n",
    "            assert X_original.shape[2] == len(select_feature_list)\n",
    "\n",
    "        Y = np.load(f\"{data_path}/{data_type}_Y_{task_name}.npy\", allow_pickle=True)\n",
    "        \n",
    "        if use_upsample:\n",
    "            if task_name == 'DNR' and data_type != 'test':\n",
    "                X_scalar,X_original,Y = upsampling_auto(X_scalar,X_original,Y,10)\n",
    "                \n",
    "        dataset_dict[task_name] = MyDataset(X_scalar,X_original,Y)\n",
    "    \n",
    "    #Dataloader\n",
    "    loader_dict = {}\n",
    "    for key, dataset in dataset_dict.items():        \n",
    "        loader_dict[key] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataset_dict,loader_dict,feature_name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70496de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTL_to_STL(multi_task_model):\n",
    "    single_task_models = {}\n",
    "\n",
    "    for task_index, task_name in enumerate(multi_task_model.task_name_list):\n",
    "        \n",
    "        single_task_model = RNN_MTL(input_dim, [task_name])\n",
    "\n",
    "        print(input_dim)  \n",
    "        single_task_model.to(device) \n",
    "\n",
    "        #Bottom\n",
    "        # 複製bi_lstm的權重和偏差到model2的對應層中\n",
    "        single_task_model.bi_lstm.weight_ih_l0.data = multi_task_model.bi_lstm.weight_ih_l0.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l0.data = multi_task_model.bi_lstm.weight_hh_l0.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l0.data = multi_task_model.bi_lstm.bias_ih_l0.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l0.data = multi_task_model.bi_lstm.bias_hh_l0.data.clone()\n",
    "\n",
    "        single_task_model.bi_lstm.weight_ih_l1.data = multi_task_model.bi_lstm.weight_ih_l1.data.clone()\n",
    "        single_task_model.bi_lstm.weight_hh_l1.data = multi_task_model.bi_lstm.weight_hh_l1.data.clone()\n",
    "        single_task_model.bi_lstm.bias_ih_l1.data = multi_task_model.bi_lstm.bias_ih_l1.data.clone()\n",
    "        single_task_model.bi_lstm.bias_hh_l1.data = multi_task_model.bi_lstm.bias_hh_l1.data.clone()\n",
    "\n",
    "        #Tower\n",
    "        single_task_model.task_fc0[0].weight.data = multi_task_model.task_fc0[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc0[0].bias.data = multi_task_model.task_fc0[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_model.task_fc1[0].weight.data = multi_task_model.task_fc1[task_index].weight.data.clone()\n",
    "        single_task_model.task_fc1[0].bias.data = multi_task_model.task_fc1[task_index].bias.data.clone()\n",
    "\n",
    "        single_task_models[task_name] = single_task_model\n",
    "    return single_task_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a371a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    experiment_time\n",
    "    max_epoch\n",
    "    learning_rate\n",
    "    input_dim\n",
    "    task_name_list\n",
    "    train_loader_dict\n",
    "    val_dataset_dict\n",
    "    test_dataset_dict\n",
    "    device\n",
    "    is_show\n",
    "\n",
    "Output:\n",
    "    df_grade\n",
    "    stl_model_dict\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_and_test_model(experiment_time, max_epoch, learning_rate, input_dim, task_name_list, train_loader_dict, val_dataset_dict, test_dataset_dict, device,is_show = True):\n",
    "    df_grade = pd.DataFrame(columns=['time', 'task', 'auroc', 'acc', 'f1', 'pre', 'recall', 'brier_score', 'loss'])\n",
    "    best_model_params = {}\n",
    "    global_best_AUC = 0\n",
    "    global_best_loss = 10000\n",
    "    best_model_dict = {} \n",
    "    \n",
    "    count = 1\n",
    "    local_indicator = 'auroc'\n",
    "    global_indicator = 'loss'\n",
    "    \n",
    "\n",
    "    \n",
    "    for time in range(experiment_time):\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        auc_list = []    \n",
    "        local_best_AUC = 0\n",
    "        local_best_loss = 10000\n",
    "        local_best_model_dict = {} \n",
    "        patience_counter = 0\n",
    "        \n",
    "        model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "        for epoch in tqdm(range(max_epoch)):\n",
    "            if is_show:\n",
    "                print(f'Time:{time+1}/{experiment_time} - Epoch:{epoch+1}/{max_epoch}...')\n",
    "                \n",
    "            train_loss = train(model, train_loader_dict, loss_func, optimizer)\n",
    "            val_score_dict, result = test(model, val_dataset_dict, loss_func, is_show= is_show)\n",
    "            \n",
    "            train_loss_list.append(train_loss)\n",
    "            val_loss_list.append(result['total_loss'])\n",
    "            auc_list.append(result['total_auc'])\n",
    "            \n",
    "            ########################################################################################################################\n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in local_best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = model.state_dict().copy()\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    local_best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            local_best_model_dict[task_name]['id'] = count                           \n",
    "                    else:\n",
    "                        if local_best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            local_best_model_dict[task_name]['performance'] = target_score\n",
    "                            local_best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            local_best_model_dict[task_name]['id'] = count\n",
    "                    \n",
    "            for task_name in task_name_list:\n",
    "                if task_name not in best_model_dict:\n",
    "                    model_dict = {}\n",
    "                    model_dict['model'] = model.state_dict().copy()\n",
    "                    model_dict['performance'] = val_score_dict[task_name]\n",
    "                    model_dict['id'] = count\n",
    "                    best_model_dict[task_name] = model_dict\n",
    "                else:\n",
    "                    target_score = val_score_dict[task_name]\n",
    "                    if local_indicator == 'auroc':\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] < target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "                    else:\n",
    "                        if best_model_dict[task_name]['performance'][local_indicator] > target_score[local_indicator] :\n",
    "                            best_model_dict[task_name]['performance'] = target_score\n",
    "                            best_model_dict[task_name]['model'] = model.state_dict().copy()\n",
    "                            best_model_dict[task_name]['id'] = count\n",
    "            count+=1\n",
    "\n",
    "            ########################################################################################################################\n",
    "            \"\"\" Early stop \"\"\"\n",
    "            if global_indicator == 'loss':\n",
    "                if result['total_loss'] < local_best_loss:\n",
    "                    local_best_loss = result['total_loss']\n",
    "                    if local_best_loss < global_best_loss:\n",
    "                        global_best_loss = local_best_loss\n",
    "                        best_model_params = model.state_dict().copy() \n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1     \n",
    "            else:\n",
    "                if result['total_auc'] > local_best_AUC:\n",
    "                    local_best_AUC = result['total_auc']\n",
    "                    if local_best_AUC > global_best_AUC:\n",
    "                        global_best_AUC = local_best_AUC\n",
    "                        best_model_params = model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1 \n",
    "            \n",
    "            global limit_early_stop_count\n",
    "            if patience_counter >= limit_early_stop_count:\n",
    "                break\n",
    "        \n",
    "        #is_show = True\n",
    "        empty_model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        test_score_dict, result = test2(local_best_model_dict, empty_model, test_dataset_dict, loss_func, is_show = is_show)\n",
    "        ########################################################################################################################\n",
    "        #input()\n",
    "        for task_name in task_name_list:\n",
    "            test_score_dict[task_name]['time'] = time + 1\n",
    "            df_grade = pd.concat([df_grade, pd.DataFrame.from_records([test_score_dict[task_name]])])\n",
    "        print(df_grade)\n",
    "        print('----------finished----------')\n",
    "    \n",
    "    \n",
    "    global_stl_model_dict = {}\n",
    "    for task_name in task_name_list:\n",
    "        model = RNN_MTL(input_dim, task_name_list).to(device)\n",
    "        model.load_state_dict(best_model_dict[task_name]['model'])\n",
    "        local_stl_model_dict = MTL_to_STL(model)\n",
    "        global_stl_model_dict[task_name] = local_stl_model_dict[task_name]\n",
    "    \n",
    "    return df_grade, global_stl_model_dict, best_model_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Input:\n",
    "    shap_value: flatten的結果,[sample,feature]\n",
    "    feature_name_list: flatten的feature list\n",
    "Output:\n",
    "    feature_important \n",
    "    sum_per_feature \n",
    "    ++\n",
    "\"\"\"\n",
    "def calculate_feature_important(shap_value,feature_name_list,seq_day, n_temporal, n_static):\n",
    "    #特徵+入時序 ex. PEEP_D1 PEEP_D2 ... \n",
    "    full_feature_name_list = []\n",
    "    for i in range(len(feature_name_list)):\n",
    "        name = feature_name_list[i]\n",
    "        #static\n",
    "        if i >=n_temporal:\n",
    "            full_feature_name_list.append(f'{name}')\n",
    "        #temporal\n",
    "        else:\n",
    "            for day in range(seq_day):\n",
    "                full_feature_name_list.append(f'{name}_D{day+1}')\n",
    "                         \n",
    "    abs_shap_value = np.abs(shap_value)\n",
    "    sum_per_feature = np.sum(abs_shap_value, axis=0) #feature,value\n",
    "    \n",
    "    assert len(full_feature_name_list) == sum_per_feature.shape[0], f'{len(full_feature_name_list)}//{sum_per_feature.shape[0]}'\n",
    "    \n",
    "    #未排序\n",
    "    feature_important_dict = dict(zip(full_feature_name_list, sum_per_feature))\n",
    "    \n",
    "    sorted_feature_indices = np.argsort(sum_per_feature)[::-1] #[::-1]是reversed\n",
    "    sorted_feature_names = [full_feature_name_list[i] for i in sorted_feature_indices]\n",
    "    return sorted_feature_names, sum_per_feature, feature_important_dict\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    model  \n",
    "    train_X (np)\n",
    "    test_X  (np)\n",
    "    test_X_original (np)\n",
    "    feature_name_list (list)\n",
    "    task_name (string)\n",
    "    use_mini_sample (是否少量資料計算shap)\n",
    "    n_sample, (shap參數)\n",
    "    n_temporal,\n",
    "    n_static \n",
    "    \n",
    "Output:\n",
    "    shap_value\n",
    "    shap_data\n",
    "    (flatten的結果)\n",
    "\"\"\"\n",
    "#####################################################################################\n",
    "def get_model_shap(model,data_X_train,data_X_test,data_X_test_original,feature_name_list,task_name,use_mini_sample = True,n_sample = 100,n_temporal = 103,n_static = 53):\n",
    "    max_sample = 1000\n",
    "    seq_day = data_X_train.shape[1]\n",
    "    feature_count = data_X_train.shape[2]\n",
    "    \n",
    "    if use_mini_sample:\n",
    "        background_data = torch.from_numpy(data_X_train[:max_sample]).float().to(device)\n",
    "        shap_data = torch.from_numpy(data_X_test[:max_sample]).float().to(device)\n",
    "        shap_data_original = torch.from_numpy(data_X_test_original[:max_sample]).float().to(device)\n",
    "    else:\n",
    "        background_data = torch.from_numpy(data_X_train[:]).float().to(device)\n",
    "        shap_data = torch.from_numpy(data_X_test[:]).float().to(device)\n",
    "        shap_data_original = torch.from_numpy(data_X_test_original[:]).float().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    explainer = shap.GradientExplainer(model, background_data)\n",
    "    \n",
    "    shap_values = explainer.shap_values(shap_data,nsamples=n_sample)\n",
    "    shap_values = np.array(shap_values)\n",
    "    \n",
    "    shap_value_flatten = np.zeros((len(shap_data),seq_day*n_temporal + n_static))\n",
    "    shap_data_flatten = np.zeros((len(shap_data),seq_day*n_temporal + n_static))\n",
    "    \n",
    "    for i in range(0,len(shap_data)):\n",
    "        count=0\n",
    "        for j in range(feature_count):\n",
    "            #static\n",
    "            if j >= n_temporal:\n",
    "                for k in range(seq_day):\n",
    "                    shap_value_flatten[i][count]=shap_values[i][k][j]  \n",
    "                    shap_data_flatten[i][count]=shap_data_original[i][k][j]  \n",
    "                shap_value_flatten[i][count]/= seq_day\n",
    "                shap_data_flatten[i][count]/= seq_day\n",
    "                count += 1\n",
    "            #temporal\n",
    "            else:\n",
    "                for k in range(seq_day):\n",
    "                    shap_value_flatten[i][count]=shap_values[i][k][j]  \n",
    "                    shap_data_flatten[i][count]=shap_data_original[i][k][j]  \n",
    "                    count += 1\n",
    "            \n",
    "    feature_important, _, feature_important_dict = calculate_feature_important(shap_value_flatten, feature_name_list, seq_day, n_temporal, n_static)\n",
    "    return feature_important, shap_value_flatten, shap_data_flatten, feature_important_dict\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    shap_value_flatten (sample,feature_flatten)\n",
    "    shap_data_flatten (sample,feature_flatten)\n",
    "    max_display \n",
    "\"\"\"\n",
    "def show_shap(shap_value_flatten, shap_data_flatten,feature_name_list, max_display = 20,task_name = '',plot_type = 'dot'):\n",
    "    fig = shap.summary_plot(shap_value_flatten,shap_data_flatten,feature_names=feature_name_list,plot_type = plot_type, show=False,max_display = max_display)\n",
    "    # plt.title(f\"***Task:{task_name}***\")\n",
    "    # plt.xticks(fontsize=20, fontweight='bold', fontfamily='Arial')\n",
    "    # plt.yticks(fontsize=20, fontweight='bold', fontfamily='Arial')\n",
    "    # plt.xlabel('SHAP Value',fontsize=24, fontweight='bold', fontfamily='Arial')\n",
    "    # plt.ylabel('Feature',fontsize=26, fontweight='bold', fontfamily='Arial')\n",
    "    \n",
    "    # ax = plt.gca()  # 获取当前图形的轴\n",
    "    # #plt.savefig(f'./解釋用模型/解釋結果/SHAP_{plot_type}.tif', bbox_inches = 'tight', dpi=300)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31150f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8e612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cbcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select_feature_list = []\n",
    "top_percent = 0.9\n",
    "remove_time_count = 0\n",
    "full_result_dict = {}\n",
    "select_feature_list = []\n",
    "\n",
    "\"\"\"\n",
    "read data\n",
    "\"\"\"\n",
    "train_dataset_dict,train_loader_dict,feature_name_list = read_data(task_name_list,'','train',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "val_dataset_dict,val_loader_dict,_ = read_data(task_name_list,'','validation',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "test_dataset_dict,test_loader_dict,_ = read_data(task_name_list,'','test',select_feature_list,batch_size = batch_size,use_upsample = use_upsample)\n",
    "if len(select_feature_list)!=0:\n",
    "    feature_name_list = select_feature_list\n",
    "input_dim = train_dataset_dict[task_name_list[0]].inputs.numpy().shape[2]\n",
    "print(f'==> input_dim: {input_dim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=103 : gender\n",
    "for i in range(len(feature_name_list)):\n",
    "    print(f'{i}...{feature_name_list[i]}')\n",
    "#input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = BCEFocalLoss(alpha=alpha, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d0b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f06fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++\n",
    "stl_model_dict = {}\n",
    "\n",
    "for task_name in task_name_list:\n",
    "    model = RNN_MTL(input_dim,[task_name]).to(device)\n",
    "    route = ''\n",
    "    if len(task_name_list) == 1:\n",
    "        #route = f'model_parm/STL/{task_name}'\n",
    "        \n",
    "        route = f'C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/STL/{task_name}'\n",
    "       \n",
    "        \n",
    "    else:\n",
    "        #route = 'model_parm/MTL/'+('+'.join(task_name_list)) \n",
    "        route = 'C:/Users/USER/M1326168/MIMIC/DNR/20250507/model_parm/MTL/'+('+'.join(task_name_list))\n",
    "        \n",
    "        \n",
    "         \n",
    "    #model.load_state_dict(torch.load(f'{route}/{task_name}_1'))\n",
    "    model.load_state_dict(torch.load(f'{route}/{task_name}_1'))  \n",
    "    stl_model_dict[task_name] = model\n",
    "    \n",
    "    task_dict = {}\n",
    "    task_dict[task_name] = test_dataset_dict[task_name]\n",
    "    result,_ = test(model, task_dict, loss_func, is_show = False)\n",
    "    print(result[task_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shap+++\n",
    "\"\"\"\n",
    "shap_dict = {}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "n_temporal = 112\n",
    "n_static = 142\n",
    "\"\"\"\n",
    "# n_temporal = 112\n",
    "# n_static = 142\n",
    "\n",
    "#n_temporal = 128\n",
    "#n_static = 145\n",
    "\n",
    "#n_temporal = 128\n",
    "#n_static = 144\n",
    "\n",
    "# n_temporal = 128\n",
    "# n_static = 143\n",
    "\n",
    "#20250507\n",
    "n_temporal = 126\n",
    "n_static = 145\n",
    "\n",
    "assert input_dim == n_temporal + n_static, f'{input_dim}!={n_temporal}+{n_static}'\n",
    "\n",
    "seq_day = 3\n",
    "feature_count = len(feature_name_list)\n",
    "sum_shap_value = np.zeros((0, seq_day * n_temporal + n_static))\n",
    "sum_shap_data = np.zeros((0, seq_day * n_temporal + n_static))\n",
    "\n",
    "#計算各任務的feature_important\n",
    "\"\"\"\n",
    "shap_dict\n",
    "    - task_name\n",
    "        - feature_important\n",
    "        - shap_value\n",
    "        - shap_data\n",
    "        - feature_name_list\n",
    "\"\"\"\n",
    "for task_name in task_name_list:\n",
    "    shap_dict[task_name] = {}\n",
    "    feature_important, shap_value_flatten, shap_data_flatten, feature_important_dict = get_model_shap(\n",
    "                                                                                        stl_model_dict[task_name],\n",
    "                                                                                        train_dataset_dict[task_name].inputs.numpy(),\n",
    "                                                                                        test_dataset_dict[task_name].inputs.numpy(),\n",
    "                                                                                        test_dataset_dict[task_name].inputs_original.numpy(),\n",
    "                                                                                        feature_name_list,\n",
    "                                                                                        task_name,\n",
    "                                                                                        use_mini_sample = use_mini_sample,\n",
    "                                                                                        n_sample = 1,\n",
    "                                                                                        n_temporal = n_temporal,\n",
    "                                                                                        n_static = n_static)\n",
    "\n",
    "    shap_dict[task_name]['feature_important'] = feature_important\n",
    "    shap_dict[task_name]['shap_value'] = shap_value_flatten\n",
    "    shap_dict[task_name]['shap_data'] = shap_data_flatten\n",
    "    shap_dict[task_name]['feature_name_list'] = feature_name_list\n",
    "    shap_dict[task_name]['feature_important_dict'] = feature_important_dict\n",
    "\n",
    "    sum_shap_value = np.vstack([sum_shap_value, shap_value_flatten])\n",
    "    sum_shap_data = np.vstack([sum_shap_data, shap_data_flatten]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38275c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature名稱+日期\n",
    "full_feature_name_list = []\n",
    "for i in range(len(feature_name_list)):\n",
    "    name = feature_name_list[i]\n",
    "    #static\n",
    "    if i >=n_temporal:\n",
    "        full_feature_name_list.append(f'{name}')\n",
    "    #temporal\n",
    "    else:\n",
    "        for day in range(seq_day):\n",
    "            full_feature_name_list.append(f'{name}_D{day+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarry plot\n",
    "for task_name in task_name_list:\n",
    "    if task_name == 'DNR':\n",
    "        shap_value = shap_dict[task_name]['shap_value']\n",
    "        shap_data = shap_dict[task_name]['shap_data']\n",
    "        print(shap_value.shape)\n",
    "        print(f'Task: {task_name}')\n",
    "        show_shap(shap_value, shap_data,full_feature_name_list,task_name = task_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c210685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summary plot\n",
    "# for task_name in task_name_list:\n",
    "#     if task_name == 'DNR':\n",
    "#         if task_name not in shap_dict:\n",
    "#             print(f\"Task '{task_name}' not found in shap_dict.\")\n",
    "#             continue\n",
    "        \n",
    "#         shap_value = shap_dict[task_name].get('shap_value')\n",
    "#         shap_data = shap_dict[task_name].get('shap_data')\n",
    "        \n",
    "#         if shap_value is None or shap_data is None:\n",
    "#             print(f\"Missing SHAP values or data for task: {task_name}\")\n",
    "#             continue\n",
    "        \n",
    "#         # 檢查 shap_value 和 shap_data 的形狀是否一致\n",
    "#         if shap_value.shape != shap_data.shape:\n",
    "#             print(f\"Shape mismatch for task '{task_name}': shap_value {shap_value.shape}, shap_data {shap_data.shape}\")\n",
    "#             continue\n",
    "        \n",
    "#         print(shap_value.shape)\n",
    "#         print(f'Task: {task_name}')\n",
    "        \n",
    "#         # 呼叫 show_shap 繪製圖表\n",
    "#         show_shap(shap_value, shap_data, full_feature_name_list, task_name=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dfb0f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#所有PDP\n",
    "task_name = 'DNR'\n",
    "for i in range (len(full_feature_name_list)):\n",
    "    print(f'Task:{task_name}...ID[{i}]')\n",
    "    shap.dependence_plot(i, shap_dict[task_name]['shap_value'], shap_dict[task_name]['shap_data'], feature_names=full_feature_name_list, interaction_index=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pdp_plot(x, y, feature_name, point_color='black'):\n",
    "#     point_size = 4\n",
    "#     #plt.figure(figsize=(6,4))\n",
    "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#     plt.scatter(x, y, color=point_color,s=point_size)\n",
    "#     #plt.xlabel(feature_name)\n",
    "#     plt.xlabel(feature_name, fontsize=26, fontweight='bold', fontfamily='Arial')\n",
    "#     plt.ylabel('SHAP Value', fontsize=26, fontweight='bold', fontfamily='Arial')\n",
    "#     plt.tick_params(axis='both', which='both', labelsize=18)\n",
    "#     plt.legend()\n",
    "    \n",
    "#     if feature_name == 'Peak Airway Pressure':\n",
    "#         plt.xticks([10,13,16,19,22,25], [10,13,16,19,22,25])\n",
    "#     if feature_name == 'RASS':\n",
    "#         plt.xticks([-5,-4,-3,-2,-1,0,1,2,3], [-5,-4,-3,-2,-1,0,1,2,3])\n",
    "#     if feature_name == 'FiO2':\n",
    "#         plt.xticks([20,40,60,80,100], [20,40,60,80,100])\n",
    "#     plt.xticks(fontweight='bold')\n",
    "#     plt.yticks([])\n",
    "#     plt.legend().set_visible(False)\n",
    "#     #plt.legend(loc='lower center')\n",
    "#     #plt.savefig(f'./PDP/{feature_name}.png', bbox_inches = 'tight', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "# for feature_name in feature_important[:7]:\n",
    "#     indedx_of_feature = feature_name_list.index(feature_name)\n",
    "    \n",
    "#     shap_data,shap_value = data_select(feature_name,shap_data_flatten,shap_value_flatten,indedx_of_feature)\n",
    "\n",
    "#     print(feature_name)\n",
    "\n",
    "#     # if feature_name == 'apsiii':\n",
    "#     #     name = 'APACHE III'\n",
    "#     # elif feature_name == 'total':\n",
    "#     #     name = 'Fluid balance'\n",
    "#     # elif feature_name == 'Nutrition_Enteral_value':\n",
    "#     #     name = 'Enteral feeding'\n",
    "#     # else:\n",
    "#     #     name = feature_name\n",
    "        \n",
    "#     pdp_plot(\n",
    "#         shap_data, \n",
    "#         shap_value, \n",
    "#         name\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc589d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature_name in feature_important[:7]:\n",
    "#     if feature_name not in feature_name_list:\n",
    "#         print(f\"Warning: '{feature_name}' not found in feature_name_list. Skipping...\")\n",
    "#         continue  # 跳過不存在的特徵名稱\n",
    "\n",
    "#     indedx_of_feature = feature_name_list.index(feature_name)\n",
    "\n",
    "#     shap_data, shap_value = data_select(feature_name, shap_data_flatten, shap_value_flatten, indedx_of_feature)\n",
    "\n",
    "#     print(feature_name)\n",
    "\n",
    "#     # 繪製 PDP 圖\n",
    "#     pdp_plot(\n",
    "#         shap_data,\n",
    "#         shap_value,\n",
    "#         feature_name\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shap_dict['DNR']['feature_important'])):\n",
    "    print(f\"{i+1}....{shap_dict['DNR']['feature_important'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#各特徵索引\n",
    "for i in range (len(full_feature_name_list)):\n",
    "    print(f'[{i}]...{full_feature_name_list[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b9811",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "輸入索引查詢對應PDP\n",
    "\"\"\"\n",
    "choose_feature_index = int(input())\n",
    "shap.dependence_plot(choose_feature_index, shap_dict[task_name]['shap_value'], shap_dict[task_name]['shap_data'], feature_names=full_feature_name_list, interaction_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d437c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDP反轉，黑白版\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 字體屬性設定\n",
    "font_properties = {'size': 24, 'family': 'Arial', 'fontweight': 'bold'}\n",
    "\n",
    "choose_feature_index = int(input(\"請輸入特徵索引：\"))\n",
    "\n",
    "# 提取對應的特徵值和 SHAP 值\n",
    "feature_values = shap_dict[task_name]['shap_data'][:, choose_feature_index]\n",
    "shap_values = shap_dict[task_name]['shap_value'][:, choose_feature_index]\n",
    "\n",
    "# 根據特徵值進行降序排序\n",
    "sorted_indices = np.argsort(feature_values)[::-1]  # 降序排序\n",
    "sorted_feature_values = feature_values[sorted_indices]\n",
    "sorted_shap_values = shap_values[sorted_indices]\n",
    "\n",
    "# 繪製 SHAP 依賴圖（反轉 X 軸）\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sorted_feature_values, sorted_shap_values, alpha=0.5, color='black', s=10)\n",
    "#plt.gca().set_xlim(max(feature_values), min(feature_values))  # 設定 X 軸範圍為降序\n",
    "plt.gca().set_xlim(2000, 0, 500)  # 設定 X 軸範圍為 0~2000，並反轉\n",
    "\n",
    "plt.xlabel(full_feature_name_list[choose_feature_index], fontsize=font_properties['size'], \n",
    "           fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "plt.ylabel(f\"SHAP Value\", fontsize=font_properties['size'], \n",
    "           fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "plt.xticks(fontsize=font_properties['size'], fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "plt.yticks(fontsize=font_properties['size'], fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "plt.show()\n",
    "\n",
    "#Urine_value_D3 = 158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDP圖反轉\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "choose_feature_index = int(input())\n",
    "\n",
    "# 提取對應的特徵值和 SHAP 值\n",
    "feature_values = shap_dict[task_name]['shap_data'][:, choose_feature_index]\n",
    "shap_values = shap_dict[task_name]['shap_value'][:, choose_feature_index]\n",
    "\n",
    "# 根據特徵值進行降序排序\n",
    "sorted_indices = np.argsort(feature_values)[::-1]  # 降序排序\n",
    "sorted_feature_values = feature_values[sorted_indices]\n",
    "sorted_shap_values = shap_values[sorted_indices]\n",
    "\n",
    "# 繪製 SHAP 依賴圖（反轉 X 軸）\n",
    "plt.scatter(sorted_feature_values, sorted_shap_values, s=20)\n",
    "plt.gca().set_xlim(max(feature_values), min(feature_values))  # 設定 X 軸範圍為降序\n",
    "plt.xlabel(full_feature_name_list[choose_feature_index])\n",
    "plt.ylabel(f'SHAP value for {full_feature_name_list[choose_feature_index]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c4044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range (len(full_feature_name_list)):\n",
    "    if full_feature_name_list[i] in shap_dict[task_name]['feature_important'][:20]:\n",
    "        print(f'Task:{task_name}...ID[{i}]')\n",
    "        #shap.dependence_plot(i, shap_dict[task_name]['shap_value'], shap_dict[task_name]['shap_data'], feature_names=full_feature_name_list, interaction_index=None)\n",
    "        shap.dependence_plot(i, shap_dict[task_name]['shap_value'], shap_dict[task_name]['shap_data'], feature_names=full_feature_name_list, interaction_index=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#前20個PDP，黑白版\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_pdp(task_name, shap_dict, full_feature_name_list, top_k=10):\n",
    "    \"\"\"\n",
    "    繪製指定任務的前 K 個重要特徵的部分依賴圖 (PDP)。\n",
    "\n",
    "    Args:\n",
    "        task_name (str): 任務名稱。\n",
    "        shap_dict (dict): 包含 SHAP 值和相關數據的字典。\n",
    "        full_feature_name_list (list): 完整的特徵名稱列表。\n",
    "        top_k (int): 要繪製的前 K 個重要特徵數量。\n",
    "    \"\"\"\n",
    "    # 確保 task_name 存在於 shap_dict 中\n",
    "    if task_name not in shap_dict:\n",
    "        print(f\"Task '{task_name}' not found in shap_dict.\")\n",
    "        return\n",
    "\n",
    "    # 獲取該任務的前 K 個重要特徵\n",
    "    top_features = shap_dict[task_name]['feature_important'][:top_k]\n",
    "\n",
    "    font_properties = {'size': 24, 'family': 'Arial', 'fontweight': 'bold'}\n",
    "\n",
    "    for feature_name in top_features:\n",
    "        feature_index = full_feature_name_list.index(feature_name)\n",
    "        shap_values = shap_dict[task_name]['shap_value'][:, feature_index]\n",
    "        feature_values = shap_dict[task_name]['shap_data'][:, feature_index]\n",
    "\n",
    "        # 繪製 PDP\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(feature_values, shap_values, alpha=0.5, color='black', s=10)\n",
    "        plt.xlabel(feature_name, fontsize=font_properties['size'], fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "        plt.ylabel('SHAP Value', fontsize=font_properties['size'], fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "        plt.xticks(fontsize=font_properties['size'], fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "        plt.yticks(fontsize=font_properties['size'], fontweight=font_properties['fontweight'], fontfamily=font_properties['family'])\n",
    "        #plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "# 使用範例\n",
    "plot_pdp(task_name='DNR', shap_dict=shap_dict, full_feature_name_list=full_feature_name_list, top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c5ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8a6bf63",
   "metadata": {},
   "source": [
    "# Feature Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_full_feature_name_with_category = pd.read_csv('./data/sample/full_feature_name_with_category.csv')\n",
    "\n",
    "df_full_feature_name_with_category = pd.read_csv('C:/Users/USER/M1326168/MIMIC/DNR/20250507/data/sample/full_feature_name_with_category.csv')\n",
    "print(df_full_feature_name_with_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ed69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將相同 category 的值相加\n",
    "result = {}\n",
    "for key, value in shap_dict['DNR']['feature_important_dict'].items():\n",
    "    for index, row in df_full_feature_name_with_category.iterrows():\n",
    "        if key.startswith(row['feature_name']):\n",
    "            category = row['category']\n",
    "            if category not in result:\n",
    "                result[category] = 0\n",
    "            result[category] += value\n",
    "            break\n",
    "#轉百分比\n",
    "total_sum = sum(result.values())\n",
    "result_percentage = {category: (value / total_sum) * 100 for category, value in result.items()}\n",
    "#print(result_percentage)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(list(result_percentage.keys()), list(result_percentage.values()))\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Feature Important(%)')\n",
    "#plt.title('Sum of Values by Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_important_dict = {}\n",
    "for feature_name in feature_name_list:\n",
    "    total_importance = 0\n",
    "    for key, value in shap_dict['DNR']['feature_important_dict'].items():\n",
    "        if key.startswith(feature_name):\n",
    "            total_importance += value\n",
    "    group_important_dict[feature_name] = total_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac86bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依重要度排序\n",
    "top_k = 15\n",
    "sorted_group_important_dict = dict(sorted(group_important_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# 繪製重要度圖\n",
    "top_features = list(sorted_group_important_dict.keys())[:top_k]\n",
    "top_importance = [sorted_group_important_dict[feature] for feature in top_features]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features, top_importance)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f'Top {top_k} Feature Importance')\n",
    "plt.gca().invert_yaxis()  # 使得重要度高的特徵在上方\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d7365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22f35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21d56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d12540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
